{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Documentation Getting Started Introduction Latest Release Configuration Windows Installation Guide Model Setup Linux (Docker) Full Installation Guide Quick Installation Model Setup Tutorial Llama3-linux Llama3-windows Bot Setup Gemini API Key Application DB/QA RAG Setup SearchQA Setup Stable Diffusion Image Generation Model Building Visual-Language Model Setup Whisper Setup RAG Custom Parameters Tool Development Cool-Whisper Setup RAG Toolchain Multi-chat Model Serving Client Library Resources FAQ Troubleshooting Guide Community Contribution Guidelines Legal Information","title":"Documentation"},{"location":"#documentation","text":"","title":"Documentation"},{"location":"#getting-started","text":"Introduction Latest Release Configuration","title":"Getting Started"},{"location":"#windows","text":"Installation Guide Model Setup","title":"Windows"},{"location":"#linux-docker","text":"Full Installation Guide Quick Installation Model Setup","title":"Linux (Docker)"},{"location":"#tutorial","text":"Llama3-linux Llama3-windows Bot Setup Gemini API Key Application DB/QA RAG Setup SearchQA Setup Stable Diffusion Image Generation Model Building Visual-Language Model Setup Whisper Setup RAG Custom Parameters Tool Development Cool-Whisper Setup RAG Toolchain Multi-chat Model Serving Client Library","title":"Tutorial"},{"location":"#resources","text":"FAQ Troubleshooting Guide Community Contribution Guidelines Legal Information","title":"Resources"},{"location":"community/","text":"Community Community Contribution L.J. Chen, \"Kuwa AI Installation Process Notes\" S.L. Hsu, \"Kuwa GenAI OS Windows \u53ef\u651c\u5f0f\u7248\u672c\u8abf\u6821\u7b46\u8a18\" W.C. Wu, \"Kuwa User Manual on Linux\" Group Kuwa AI Kuwa AI Community Kuwa AI Taiwan Community kuwa-dev@googlegroups.com Announcement Kuwa AI kuwa-announce@googlegroups.com","title":"Community"},{"location":"community/#community","text":"","title":"Community"},{"location":"community/#community-contribution","text":"L.J. Chen, \"Kuwa AI Installation Process Notes\" S.L. Hsu, \"Kuwa GenAI OS Windows \u53ef\u651c\u5f0f\u7248\u672c\u8abf\u6821\u7b46\u8a18\" W.C. Wu, \"Kuwa User Manual on Linux\"","title":"Community Contribution"},{"location":"community/#group","text":"Kuwa AI Kuwa AI Community Kuwa AI Taiwan Community kuwa-dev@googlegroups.com","title":"Group"},{"location":"community/#announcement","text":"Kuwa AI kuwa-announce@googlegroups.com","title":"Announcement"},{"location":"configuration/","text":"Configuration Kuwa GenAI OS is divided into the front-end Multi-chat WebUI, the back-end Kuwa Kernel, and Kuwa Executors. This document records the setup methods for each component. Multi-chat WebUI Configuration File The main configuration file for Multi-chat is located at .env in multi-chat/ . You can refer to .env.dev in multi-chat/ to adjust the settings to meet your needs. APP_NAME=\"Kuwa Chat\" # The name of the entire website, which will be applied to the website Title, email name, etc. APP_ENV=production # Use \"local\" during development, change to \"production\" when officially launched APP_KEY= # The secret key for Cookies and Sessions, which is automatically generated by the installer and does not need to be set manually APP_DEBUG=false # Whether to print debug messages APP_URL=http://127.0.0.1/ # The publicly accessible URL for the website API_Key= # The secret key used for internal API calls, which is automatically generated by the installer and does not need to be set manually Email_Required=true # Whether to require email when logging in, usually it's best to keep it true unless you need to rewrite the login authentication method # Log settings # For more information, see: https://laravel.com/docs/10.x/logging LOG_CHANNEL=stack LOG_DEPRECATIONS_CHANNEL=null LOG_LEVEL=debug # Database settings # For more information, see: https://laravel.com/docs/10.x/database DB_CONNECTION=sqlite # DB_HOST= # DB_PORT= # DB_DATABASE= # DB_USERNAME= # DB_PASSWORD= # Email sending settings, used to send verification emails for forgotten passwords # For more information, see: https://laravel.com/docs/10.x/mail MAIL_MAILER=smtp MAIL_HOST=smtp.gmail.com MAIL_PORT=465 MAIL_USERNAME= MAIL_PASSWORD= MAIL_ENCRYPTION=tls MAIL_FROM_ADDRESS=\"YOUR MAIL HERE\" MAIL_FROM_NAME=\"${APP_NAME}\" # Service queue settings, used to store service requests sent to the backend # For more information, see: https://laravel.com/docs/10.x/redis REDIS_HOST=127.0.0.1 REDIS_PASSWORD=null REDIS_PORT=6379 # LDAP settings, used to connect to other permission management systems # For more information, see: https://ldaprecord.com/docs/laravel/v3/configuration/#using-an-environment-file-env LDAP_LOGGING=false LDAP_CONNECTION=default LDAP_HOST= LDAP_USERNAME=\"uid=root,cn=users,dc=test,dc=com\" LDAP_PASSWORD= LDAP_PORT=389 LDAP_BASE_DN=\"dc=test,dc=com\" LDAP_TIMEOUT=5 LDAP_SSL=false LDAP_TLS=false LDAP_SASL=false INFORMATION_URL='https://kuwaai.tw/' # Displayed in \"More Information\" in the menu bar, allowing users to enter quickly # ALLOWED_IPS= # Restrict the source IP addresses that can access the website, you can fill in multiple CIDRs separated by commas, including IPv4 and IPv6 # ALLOWED_EMAIL= # Restrict the email endings that can be used during registration, use commas to separate. For example, nuk.edu.tw, then mail.nuk.edu.tw, nuk.edu.tw, etc. can be registered # DEFAULT_GROUP= # The default group invitation code, if no invitation code is filled in or the invitation code does not exist, the group corresponding to this invitation code will be joined # Supported languages that can be switched on the website, filled in JSON format # For example, '{\"en_us\":\"English\",\"zh_tw\":\"Chinese (Taiwan)\"}' can limit the language switch to English and Chinese # en_us and zh_tw are the language file names under multi-chat\\lang # LANGUAGES='{\"en_us\":\"English\",\"zh_tw\":\"Chinese (Taiwan)\",\"zh_cn\":\"Chinese(China)\",\"ja_jp\":\"Japanese (Japan)\",\"de\":\"Deutsch (Deutschland)\",\"cs_cz\":\"\u010ce\u0161tina (\u010cesk\u00e1 republika)\",\"fr_fr\":\"Fran\u00e7ais (France)\",\"ko_kr\":\"\ud55c\uad6d\uc5b4 (\ub300\ud55c\ubbfc\uad6d)\"}' locale=en_us # Default language, if it is en_us, the default language is English, and if it is zh_tw, the default language is Chinese fallback_locale=en_us # To which language should the translation fallback when the language file is missing Customized Components Multi-chat supports customized UI components. You can create your own UI components under multi-chat/resources/views/components/custom/ to replace the default components with the same name under components/ . Currently, it supports customization of the main logo of the application, the homepage and the large logo of the maintenance page, the main content of the homepage, the footer content of the homepage, etc. Main Logo of the Application Component name: application-logo.blade.php Large Logo of the Homepage and Maintenance Page Component name: logo.blade.php Main Content of the Homepage Component name: welcome_body.blade.php Footer Content of the Homepage Component name: welcome_footer.blade.php Kuwa Kernel Currently, the main configuration file for the Kernel is located under genai-os/kernel/src/variable.py . version = \"v1.0\" # Kuwa Kernel version, do not modify data = {} # Internal data, do not modify log_folder = \"logs\" # Log directory record_file = \"records.pickle\" # The permanent storage location for the internal data of Kuwa Kernel, used to retain data when restarting the Kernel port = 9000 # The connection port that the Kernel listens to ip = \"0.0.0.0\" # The IP address that the Kernel listens to # The following are the related settings for the safety filter (SafetyGuard) function os.environ['SAFETY_GUARD_MANAGER_URL'] = 'http://localhost:8000' # Manager component address os.environ['SAFETY_GUARD_DETECTOR_URL'] = 'grpc://localhost:50051' # Detector component address safety_guard_update_interval_sec = 30 # Filter list update interval Kuwa Executors Please refer to the setup instructions in GitHub - Setup Guide in Chinese - Setup Guide in English","title":"Configuration"},{"location":"configuration/#configuration","text":"Kuwa GenAI OS is divided into the front-end Multi-chat WebUI, the back-end Kuwa Kernel, and Kuwa Executors. This document records the setup methods for each component.","title":"Configuration"},{"location":"configuration/#multi-chat-webui","text":"","title":"Multi-chat WebUI"},{"location":"configuration/#configuration-file","text":"The main configuration file for Multi-chat is located at .env in multi-chat/ . You can refer to .env.dev in multi-chat/ to adjust the settings to meet your needs. APP_NAME=\"Kuwa Chat\" # The name of the entire website, which will be applied to the website Title, email name, etc. APP_ENV=production # Use \"local\" during development, change to \"production\" when officially launched APP_KEY= # The secret key for Cookies and Sessions, which is automatically generated by the installer and does not need to be set manually APP_DEBUG=false # Whether to print debug messages APP_URL=http://127.0.0.1/ # The publicly accessible URL for the website API_Key= # The secret key used for internal API calls, which is automatically generated by the installer and does not need to be set manually Email_Required=true # Whether to require email when logging in, usually it's best to keep it true unless you need to rewrite the login authentication method # Log settings # For more information, see: https://laravel.com/docs/10.x/logging LOG_CHANNEL=stack LOG_DEPRECATIONS_CHANNEL=null LOG_LEVEL=debug # Database settings # For more information, see: https://laravel.com/docs/10.x/database DB_CONNECTION=sqlite # DB_HOST= # DB_PORT= # DB_DATABASE= # DB_USERNAME= # DB_PASSWORD= # Email sending settings, used to send verification emails for forgotten passwords # For more information, see: https://laravel.com/docs/10.x/mail MAIL_MAILER=smtp MAIL_HOST=smtp.gmail.com MAIL_PORT=465 MAIL_USERNAME= MAIL_PASSWORD= MAIL_ENCRYPTION=tls MAIL_FROM_ADDRESS=\"YOUR MAIL HERE\" MAIL_FROM_NAME=\"${APP_NAME}\" # Service queue settings, used to store service requests sent to the backend # For more information, see: https://laravel.com/docs/10.x/redis REDIS_HOST=127.0.0.1 REDIS_PASSWORD=null REDIS_PORT=6379 # LDAP settings, used to connect to other permission management systems # For more information, see: https://ldaprecord.com/docs/laravel/v3/configuration/#using-an-environment-file-env LDAP_LOGGING=false LDAP_CONNECTION=default LDAP_HOST= LDAP_USERNAME=\"uid=root,cn=users,dc=test,dc=com\" LDAP_PASSWORD= LDAP_PORT=389 LDAP_BASE_DN=\"dc=test,dc=com\" LDAP_TIMEOUT=5 LDAP_SSL=false LDAP_TLS=false LDAP_SASL=false INFORMATION_URL='https://kuwaai.tw/' # Displayed in \"More Information\" in the menu bar, allowing users to enter quickly # ALLOWED_IPS= # Restrict the source IP addresses that can access the website, you can fill in multiple CIDRs separated by commas, including IPv4 and IPv6 # ALLOWED_EMAIL= # Restrict the email endings that can be used during registration, use commas to separate. For example, nuk.edu.tw, then mail.nuk.edu.tw, nuk.edu.tw, etc. can be registered # DEFAULT_GROUP= # The default group invitation code, if no invitation code is filled in or the invitation code does not exist, the group corresponding to this invitation code will be joined # Supported languages that can be switched on the website, filled in JSON format # For example, '{\"en_us\":\"English\",\"zh_tw\":\"Chinese (Taiwan)\"}' can limit the language switch to English and Chinese # en_us and zh_tw are the language file names under multi-chat\\lang # LANGUAGES='{\"en_us\":\"English\",\"zh_tw\":\"Chinese (Taiwan)\",\"zh_cn\":\"Chinese(China)\",\"ja_jp\":\"Japanese (Japan)\",\"de\":\"Deutsch (Deutschland)\",\"cs_cz\":\"\u010ce\u0161tina (\u010cesk\u00e1 republika)\",\"fr_fr\":\"Fran\u00e7ais (France)\",\"ko_kr\":\"\ud55c\uad6d\uc5b4 (\ub300\ud55c\ubbfc\uad6d)\"}' locale=en_us # Default language, if it is en_us, the default language is English, and if it is zh_tw, the default language is Chinese fallback_locale=en_us # To which language should the translation fallback when the language file is missing","title":"Configuration File"},{"location":"configuration/#customized-components","text":"Multi-chat supports customized UI components. You can create your own UI components under multi-chat/resources/views/components/custom/ to replace the default components with the same name under components/ . Currently, it supports customization of the main logo of the application, the homepage and the large logo of the maintenance page, the main content of the homepage, the footer content of the homepage, etc.","title":"Customized Components"},{"location":"configuration/#main-logo-of-the-application","text":"Component name: application-logo.blade.php","title":"Main Logo of the Application"},{"location":"configuration/#large-logo-of-the-homepage-and-maintenance-page","text":"Component name: logo.blade.php","title":"Large Logo of the Homepage and Maintenance Page"},{"location":"configuration/#main-content-of-the-homepage","text":"Component name: welcome_body.blade.php","title":"Main Content of the Homepage"},{"location":"configuration/#footer-content-of-the-homepage","text":"Component name: welcome_footer.blade.php","title":"Footer Content of the Homepage"},{"location":"configuration/#kuwa-kernel","text":"Currently, the main configuration file for the Kernel is located under genai-os/kernel/src/variable.py . version = \"v1.0\" # Kuwa Kernel version, do not modify data = {} # Internal data, do not modify log_folder = \"logs\" # Log directory record_file = \"records.pickle\" # The permanent storage location for the internal data of Kuwa Kernel, used to retain data when restarting the Kernel port = 9000 # The connection port that the Kernel listens to ip = \"0.0.0.0\" # The IP address that the Kernel listens to # The following are the related settings for the safety filter (SafetyGuard) function os.environ['SAFETY_GUARD_MANAGER_URL'] = 'http://localhost:8000' # Manager component address os.environ['SAFETY_GUARD_DETECTOR_URL'] = 'grpc://localhost:50051' # Detector component address safety_guard_update_interval_sec = 30 # Filter list update interval","title":"Kuwa Kernel"},{"location":"configuration/#kuwa-executors","text":"Please refer to the setup instructions in GitHub - Setup Guide in Chinese - Setup Guide in English","title":"Kuwa Executors"},{"location":"contribution_guidelines/","text":"Contributing Guidelines Kuwa is an open-source community, welcome everyone to participate in contributing \ud83d\ude01 Where can I start contributing? Currently, Kuwa GenAI OS is still in a very early stage, you can first refer to our installation guide to try to install the whole system. If you encounter any problems during the process, that's great \ud83d\udc4d it means our document is not clear enough. If you encounter any problems, please feel free to inform us through our community , we will try our best to help you solve any difficulties. If you think any feature is cool, welcome to issue on GitHub , or contact us through our community , we are happy to discuss any cool ideas with you. Of course, if you do some features yourself and want to merge them into Kuwa GenAI OS, you are also welcome to submit Pull requests on GitHub , thank you for your contribution. \ud83c\udf89","title":"Contributing Guidelines"},{"location":"contribution_guidelines/#contributing-guidelines","text":"Kuwa is an open-source community, welcome everyone to participate in contributing \ud83d\ude01","title":"Contributing Guidelines"},{"location":"contribution_guidelines/#where-can-i-start-contributing","text":"Currently, Kuwa GenAI OS is still in a very early stage, you can first refer to our installation guide to try to install the whole system. If you encounter any problems during the process, that's great \ud83d\udc4d it means our document is not clear enough. If you encounter any problems, please feel free to inform us through our community , we will try our best to help you solve any difficulties. If you think any feature is cool, welcome to issue on GitHub , or contact us through our community , we are happy to discuss any cool ideas with you. Of course, if you do some features yourself and want to merge them into Kuwa GenAI OS, you are also welcome to submit Pull requests on GitHub , thank you for your contribution. \ud83c\udf89","title":"Where can I start contributing?"},{"location":"docker_installation/","text":"Kuwa Full Installation Guide for Linux OS version: Ubuntu 22.04 LTS 1. Install Docker Refer to Docker official installation documentation . ```sh= Uninstall conflicting packages for pkg in docker.io docker-doc docker-compose docker-compose-v2 podman-docker containerd runc; do sudo apt-get remove $pkg; done Add docker's official GPG key sudo apt-get update sudo apt-get install ca-certificates sudo install -m 0755 -d /etc/apt/keyrings curl -fsSL https://download.docker.com/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.asc sudo chmod a+r /etc/apt/keyrings/docker.asc Setup repository echo \\ \"deb [arch=\"$(dpkg --print-architecture)\" signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/ubuntu \\ \"$(. /etc/os-release && echo \"$VERSION_CODENAME\")\" stable\" | \\ sudo tee /etc/apt/sources.list.d/docker.list > /dev/null sudo apt-get update Install necessary package sudo apt-get install -y docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin Enable the service sudo systemctl --now enable docker Enable unattended-update cat <<EOT | tee /etc/apt/apt.conf.d/51unattended-upgrades-docker Unattended-Upgrade::Origins-Pattern { \"origin=Docker\"; }; EOT * Use `sudo docker run hello-world` to test if docker is installed successfully. ### 2. (Optional) Install NVIDIA Drivers ```shell= # Update and Upgrade sudo apt update sudo apt upgrade # Remove previous NVIDIA installation sudo apt autoremove nvidia* --purge sudo apt autoclean # Install Ubuntu and NVIDIA drivers ubuntu-drivers devices # get the recommended version sudo ubuntu-drivers autoinstall sudo apt install nvidia-driver-$version # Reboot sudo reboot If reboot is unsuccessful, hold down shift key, select Advanced options for Ubuntu > recovery mode > dpkg , and follow the instructions to repair broken packages. After reboot, use the command nvidia-smi to check if nvidia-driver is installed successfully. possible result: 3. (Optional) Install CUDA Toolkits Refer to NVIDIA CUDA official installation guide . ```sh= Update and Upgrade sudo apt update sudo apt upgrade Install CUDA toolkit sudo apt install nvidia-cuda-toolkit Check CUDA install nvcc --version ![](./img/docker_installation_2.png) You can test CUDA on Pytorch: ```sh= sudo apt-get install python3-pip sudo pip3 install virtualenv virtualenv -p py3.10 venv source venv/bin/activate # Install pytorch pip3 install torch torchvision torchaudio pip install --upgrade pip # Test python3 (In python): ```python= import torch print(torch.cuda.is_available()) # should be True t = torch.rand(10, 10).cuda() print(t.device) # should be CUDA expected result: ![](./img/docker_installation_3.png) ### 4. (Optional) Install NVIDIA Container Toolkit Refer to [NVIDIA Container Toolkit official installation guide](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html). ```sh= # Setup GPG key curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg # Setup the repository distribution=$(. /etc/os-release;echo $ID$VERSION_ID) curl -s -L https://nvidia.github.io/libnvidia-container/$distribution/libnvidia-container.list | \\ sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | \\ sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list sudo apt-get update sudo apt-get install -y nvidia-container-toolkit # Configure the NVIDIA runtime to be the default docker runtime sudo nvidia-ctk runtime configure --runtime=docker --set-as-default sudo systemctl restart docker 5. Install Kuwa Download Kuwa Repository ```sh= git clone https://github.com/kuwaai/genai-os/ cd genai-os/docker 2. Change Configuration Files Copy `.admin-password.sample`, `.db-password.sample`, `.env.sample`, `run.sh.sample`, remove the `.sample` suffix to setup your own configuration files. ```sh= cp .admin-password.sample .admin-password cp .db-password.sample .db-password cp .env.sample .env .admin-password : default administrator password .db-password : system built-in database password .env : environment variables, the default set value is as follows DOMAIN_NAME=localhost # Website domain name, if you want to make the service public, please set it to your public domain name PUBLIC_BASE_URL=\"http://${DOMAIN_NAME}/\" # Website base URL ADMIN_NAME=\"Kuwa Admin\" # Website default administrator name ADMIN_EMAIL=\"admin@${DOMAIN_NAME}\" # Website default administrator login email, which can be an invalid email run.sh : the executable file Start the System Execute and wait for minutes. sudo ./run.sh By default, Kuwa will be deployed on http://localhost . 6. (Optional) Building Docker Images from Source Code Since version 0.3.4, Kuwa Docker Images are downloaded pre-built from Docker Hub by default. To build images from source code, follow these steps: Ensure the .git directory is present within the genai-os directory. Enable the containerd image store for multi-platform builds Add the following configuration to your /etc/docker/daemon.json file: json { \"features\": { \"containerd-snapshotter\": true } } Restart the Docker daemon after saving the changes: sh sudo systemctl restart docker 3. Build the Kuwa images using the following command: sh sudo ./run.sh build This command will create the following images: kuwaai/model-executor kuwaai/multi-chat kuwaai/kernel kuwaai/multi-chat-web","title":"Docker installation"},{"location":"docker_installation/#kuwa-full-installation-guide-for-linux","text":"OS version: Ubuntu 22.04 LTS","title":"Kuwa Full Installation Guide for Linux"},{"location":"docker_installation/#1-install-docker","text":"Refer to Docker official installation documentation . ```sh=","title":"1. Install Docker"},{"location":"docker_installation/#uninstall-conflicting-packages","text":"for pkg in docker.io docker-doc docker-compose docker-compose-v2 podman-docker containerd runc; do sudo apt-get remove $pkg; done","title":"Uninstall conflicting packages"},{"location":"docker_installation/#add-dockers-official-gpg-key","text":"sudo apt-get update sudo apt-get install ca-certificates sudo install -m 0755 -d /etc/apt/keyrings curl -fsSL https://download.docker.com/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.asc sudo chmod a+r /etc/apt/keyrings/docker.asc","title":"Add docker's official GPG key"},{"location":"docker_installation/#setup-repository","text":"echo \\ \"deb [arch=\"$(dpkg --print-architecture)\" signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/ubuntu \\ \"$(. /etc/os-release && echo \"$VERSION_CODENAME\")\" stable\" | \\ sudo tee /etc/apt/sources.list.d/docker.list > /dev/null sudo apt-get update","title":"Setup repository"},{"location":"docker_installation/#install-necessary-package","text":"sudo apt-get install -y docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin","title":"Install necessary package"},{"location":"docker_installation/#enable-the-service","text":"sudo systemctl --now enable docker","title":"Enable the service"},{"location":"docker_installation/#enable-unattended-update","text":"cat <<EOT | tee /etc/apt/apt.conf.d/51unattended-upgrades-docker Unattended-Upgrade::Origins-Pattern { \"origin=Docker\"; }; EOT * Use `sudo docker run hello-world` to test if docker is installed successfully. ### 2. (Optional) Install NVIDIA Drivers ```shell= # Update and Upgrade sudo apt update sudo apt upgrade # Remove previous NVIDIA installation sudo apt autoremove nvidia* --purge sudo apt autoclean # Install Ubuntu and NVIDIA drivers ubuntu-drivers devices # get the recommended version sudo ubuntu-drivers autoinstall sudo apt install nvidia-driver-$version # Reboot sudo reboot If reboot is unsuccessful, hold down shift key, select Advanced options for Ubuntu > recovery mode > dpkg , and follow the instructions to repair broken packages. After reboot, use the command nvidia-smi to check if nvidia-driver is installed successfully. possible result:","title":"Enable unattended-update"},{"location":"docker_installation/#3-optional-install-cuda-toolkits","text":"Refer to NVIDIA CUDA official installation guide . ```sh=","title":"3. (Optional) Install CUDA Toolkits"},{"location":"docker_installation/#update-and-upgrade","text":"sudo apt update sudo apt upgrade","title":"Update and Upgrade"},{"location":"docker_installation/#install-cuda-toolkit","text":"sudo apt install nvidia-cuda-toolkit","title":"Install CUDA toolkit"},{"location":"docker_installation/#check-cuda-install","text":"nvcc --version ![](./img/docker_installation_2.png) You can test CUDA on Pytorch: ```sh= sudo apt-get install python3-pip sudo pip3 install virtualenv virtualenv -p py3.10 venv source venv/bin/activate # Install pytorch pip3 install torch torchvision torchaudio pip install --upgrade pip # Test python3 (In python): ```python= import torch print(torch.cuda.is_available()) # should be True t = torch.rand(10, 10).cuda() print(t.device) # should be CUDA expected result: ![](./img/docker_installation_3.png) ### 4. (Optional) Install NVIDIA Container Toolkit Refer to [NVIDIA Container Toolkit official installation guide](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html). ```sh= # Setup GPG key curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg # Setup the repository distribution=$(. /etc/os-release;echo $ID$VERSION_ID) curl -s -L https://nvidia.github.io/libnvidia-container/$distribution/libnvidia-container.list | \\ sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | \\ sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list sudo apt-get update sudo apt-get install -y nvidia-container-toolkit # Configure the NVIDIA runtime to be the default docker runtime sudo nvidia-ctk runtime configure --runtime=docker --set-as-default sudo systemctl restart docker","title":"Check CUDA install"},{"location":"docker_installation/#5-install-kuwa","text":"Download Kuwa Repository ```sh= git clone https://github.com/kuwaai/genai-os/ cd genai-os/docker 2. Change Configuration Files Copy `.admin-password.sample`, `.db-password.sample`, `.env.sample`, `run.sh.sample`, remove the `.sample` suffix to setup your own configuration files. ```sh= cp .admin-password.sample .admin-password cp .db-password.sample .db-password cp .env.sample .env .admin-password : default administrator password .db-password : system built-in database password .env : environment variables, the default set value is as follows DOMAIN_NAME=localhost # Website domain name, if you want to make the service public, please set it to your public domain name PUBLIC_BASE_URL=\"http://${DOMAIN_NAME}/\" # Website base URL ADMIN_NAME=\"Kuwa Admin\" # Website default administrator name ADMIN_EMAIL=\"admin@${DOMAIN_NAME}\" # Website default administrator login email, which can be an invalid email run.sh : the executable file Start the System Execute and wait for minutes. sudo ./run.sh By default, Kuwa will be deployed on http://localhost .","title":"5. Install Kuwa"},{"location":"docker_installation/#6-optional-building-docker-images-from-source-code","text":"Since version 0.3.4, Kuwa Docker Images are downloaded pre-built from Docker Hub by default. To build images from source code, follow these steps: Ensure the .git directory is present within the genai-os directory. Enable the containerd image store for multi-platform builds Add the following configuration to your /etc/docker/daemon.json file: json { \"features\": { \"containerd-snapshotter\": true } } Restart the Docker daemon after saving the changes: sh sudo systemctl restart docker 3. Build the Kuwa images using the following command: sh sudo ./run.sh build This command will create the following images: kuwaai/model-executor kuwaai/multi-chat kuwaai/kernel kuwaai/multi-chat-web","title":"6. (Optional) Building Docker Images from Source Code"},{"location":"docker_model_setup/","text":"Model Setup for Linux/Docker 1. Gemini Gemini model is setted up by default, just enter your API key in setting > API Management . 2. ChatGPT (OPENAI) Enter genai-os/docker/compose directory and copy the .yaml file. cp gemini.yaml chatgpt.yaml Edit chatgpt.yaml: line 2: change gemini-executor to chatgpt-executor line 8: change geminipro to chatgpt line 9: change geminipro to gpt-4-turbo or other non-repeated code line 10: change Gemini to OpenAI GPT-4 or other name that will be shown as Kuwa web server chatroom name line 11: change gemini.png to chatgpt.png line 15: change command: [\"--api_key\", \u2026\u2026 to command: [\"--model\", \"gpt-4-turbo\",\"--api_key\", \"<FILL IN YOUR API key>\" Edit run.sh : Add chatgpt into confs array and execute run.sh again. confs array: chat room interface: 3. TAIDE Download the TAIDE 8B 4bit version gguf file taide-8b-a.3-q4_k_m.gguf based on Llama3 from here . ```sh= cd ~ mkdir gguf/taide curl -L -o \"taide/taide-8b-a.3-q4_k_m.gguf\" https://huggingface.co/nctu6/Llama3-TAIDE-LX-8B-Chat-Alpha1-GGUF/resolve/main/Llama3-TAIDE-LX-8B-Chat-Alpha1-Q4_K_M.gguf?download=true 2. Enter `genai-os/docker/compose` directory and copy the .yaml file. ```sh cp llamacpp.yaml llamacpp-taide.yaml Edit llamacpp-taide.yaml : line 2: change llamacpp-executor to llamacpp-taide-executor line 9: change TAIDE 4bit to llamacpp-taide or other non-repeated code line 10: change Gemini to Llama3-TAIDE-LX-8B-Chat-Alpha1-4bit or other name that will be shown as Kuwa web server chatroom name line 15: change command: [\"--model_path\", \"/var/model/taide-4bit.gguf\" ...... to command: [\"--model_path\", \"/var/model/taide-8b-a.3-q4_k_m.gguf\" ...... line 17: change /path/to/taide/model.gguf to your path to the gguf file, and change /var/model/taide-4bit.gguf to /var/model/taide-8b-a.3-q4_k_m.gguf Edit run.sh : Add llamacpp-taide into confs array and execute run.sh again. If you faced the error ModuleNotFoundError: No module named 'llama_cpp' , please refer to the commit to fix it. 4. Others, using Ollama Use Ollama, a simple API for running and managing models, to pull model. ollama pull <model name> Enter genai-os/docker/compose directory and copy the .yaml file. cp gemini.yaml ollama-<name>.yaml Edit ollama-<name>.yaml : line 2: change gemini-executor to ollama-<name>-executor line 8: change geminipro to chatgpt line 9: change geminipro to <access code> , recommended a non-repeated model version name line 10: change Gemini to <the code that will be shown in chat room interface> line 15: change command: [\"--api_key\", \u2026\u2026 to [\"--model\", \"<model name>\", \"--base_url\", \"http://host.docker.internal:11434/v1\", \"--api_key\", \"ollama\"] Note that the API URL originally prompted in Ollama is http://<localhost>:11434/v1 , but since we are using Docker to load this LLM, we can't directly access services through localhost in the container. Therefore, we should change localhost to the machine IP address or use Docker-provided IP host.docker.internal to access to services. Edit run.sh : Add <yaml file name> into confs array and execute run.sh again. 5. Others, using LM Studio Use LM Studio, a fast LLM deployment platform, to download model. Enter genai-os/docker/compose directory and copy the .yaml file. cp gemini.yaml lmstudio-<name>.yaml Edit lmstudio-<name>.yaml : line 2: change gemini-executor to lmstudio-<name>-executor line 8: change geminipro to chatgpt line 9: change geminipro to <access code> , recommended a non-repeated model version name line 10: change Gemini to <the code that will be shown in chat room interface> line 15: change command: [\"--api_key\", \u2026\u2026 to [\"--model\", \"<model name>\", \"--base_url\", \"http://host.docker.internal:11434/v1\", \"--api_key\", \"lm-studio\"] Note that the API URL originally prompted in Ollama is http://<localhost>:11434/v1 , but since we are using Docker to load this LLM, we can't directly access services through localhost in the container. Therefore, we should change localhost to the machine IP address or use Docker-provided IP host.docker.internal to access to services. Edit run.sh : Add <yaml file name> into confs array and execute run.sh again. 6. Others, using GGUF Download the gguf file. Enter genai-os/docker/compose directory and copy the .yaml file. cp llamacpp.yaml llamacpp-<name>.yaml Edit llamacpp-<name>.yaml : line 2: change llamacpp-executor to llamacpp-<name>-executor line 8: change taide-4bit to llamacpp-<name> line 9: change TAIDE 4bit to <access code> , recommended a non-repeated model version name line 10: change Gemini to <the code that will be shown in chat room interface> line 15: change command: [\"--model_path\", \"/var/model/taide-4bit.gguf\" ...... to command: [\"--model_path\", \"/var/model/<gguf file name>\" ...... line 17: change [\"/path/to/taide/model.gguf to your path to the gguf file, and change /var/model/taide-4bit.gguf to /var/model/<gguf file name> Edit run.sh : Add <yaml file name> into confs array and execute run.sh again.","title":"Docker model setup"},{"location":"docker_model_setup/#model-setup-for-linuxdocker","text":"","title":"Model Setup for Linux/Docker"},{"location":"docker_model_setup/#1-gemini","text":"Gemini model is setted up by default, just enter your API key in setting > API Management .","title":"1. Gemini"},{"location":"docker_model_setup/#2-chatgpt-openai","text":"Enter genai-os/docker/compose directory and copy the .yaml file. cp gemini.yaml chatgpt.yaml Edit chatgpt.yaml: line 2: change gemini-executor to chatgpt-executor line 8: change geminipro to chatgpt line 9: change geminipro to gpt-4-turbo or other non-repeated code line 10: change Gemini to OpenAI GPT-4 or other name that will be shown as Kuwa web server chatroom name line 11: change gemini.png to chatgpt.png line 15: change command: [\"--api_key\", \u2026\u2026 to command: [\"--model\", \"gpt-4-turbo\",\"--api_key\", \"<FILL IN YOUR API key>\" Edit run.sh : Add chatgpt into confs array and execute run.sh again. confs array: chat room interface:","title":"2. ChatGPT (OPENAI)"},{"location":"docker_model_setup/#3-taide","text":"Download the TAIDE 8B 4bit version gguf file taide-8b-a.3-q4_k_m.gguf based on Llama3 from here . ```sh= cd ~ mkdir gguf/taide curl -L -o \"taide/taide-8b-a.3-q4_k_m.gguf\" https://huggingface.co/nctu6/Llama3-TAIDE-LX-8B-Chat-Alpha1-GGUF/resolve/main/Llama3-TAIDE-LX-8B-Chat-Alpha1-Q4_K_M.gguf?download=true 2. Enter `genai-os/docker/compose` directory and copy the .yaml file. ```sh cp llamacpp.yaml llamacpp-taide.yaml Edit llamacpp-taide.yaml : line 2: change llamacpp-executor to llamacpp-taide-executor line 9: change TAIDE 4bit to llamacpp-taide or other non-repeated code line 10: change Gemini to Llama3-TAIDE-LX-8B-Chat-Alpha1-4bit or other name that will be shown as Kuwa web server chatroom name line 15: change command: [\"--model_path\", \"/var/model/taide-4bit.gguf\" ...... to command: [\"--model_path\", \"/var/model/taide-8b-a.3-q4_k_m.gguf\" ...... line 17: change /path/to/taide/model.gguf to your path to the gguf file, and change /var/model/taide-4bit.gguf to /var/model/taide-8b-a.3-q4_k_m.gguf Edit run.sh : Add llamacpp-taide into confs array and execute run.sh again. If you faced the error ModuleNotFoundError: No module named 'llama_cpp' , please refer to the commit to fix it.","title":"3. TAIDE"},{"location":"docker_model_setup/#4-others-using-ollama","text":"Use Ollama, a simple API for running and managing models, to pull model. ollama pull <model name> Enter genai-os/docker/compose directory and copy the .yaml file. cp gemini.yaml ollama-<name>.yaml Edit ollama-<name>.yaml : line 2: change gemini-executor to ollama-<name>-executor line 8: change geminipro to chatgpt line 9: change geminipro to <access code> , recommended a non-repeated model version name line 10: change Gemini to <the code that will be shown in chat room interface> line 15: change command: [\"--api_key\", \u2026\u2026 to [\"--model\", \"<model name>\", \"--base_url\", \"http://host.docker.internal:11434/v1\", \"--api_key\", \"ollama\"] Note that the API URL originally prompted in Ollama is http://<localhost>:11434/v1 , but since we are using Docker to load this LLM, we can't directly access services through localhost in the container. Therefore, we should change localhost to the machine IP address or use Docker-provided IP host.docker.internal to access to services. Edit run.sh : Add <yaml file name> into confs array and execute run.sh again.","title":"4. Others, using Ollama"},{"location":"docker_model_setup/#5-others-using-lm-studio","text":"Use LM Studio, a fast LLM deployment platform, to download model. Enter genai-os/docker/compose directory and copy the .yaml file. cp gemini.yaml lmstudio-<name>.yaml Edit lmstudio-<name>.yaml : line 2: change gemini-executor to lmstudio-<name>-executor line 8: change geminipro to chatgpt line 9: change geminipro to <access code> , recommended a non-repeated model version name line 10: change Gemini to <the code that will be shown in chat room interface> line 15: change command: [\"--api_key\", \u2026\u2026 to [\"--model\", \"<model name>\", \"--base_url\", \"http://host.docker.internal:11434/v1\", \"--api_key\", \"lm-studio\"] Note that the API URL originally prompted in Ollama is http://<localhost>:11434/v1 , but since we are using Docker to load this LLM, we can't directly access services through localhost in the container. Therefore, we should change localhost to the machine IP address or use Docker-provided IP host.docker.internal to access to services. Edit run.sh : Add <yaml file name> into confs array and execute run.sh again.","title":"5. Others, using LM Studio"},{"location":"docker_model_setup/#6-others-using-gguf","text":"Download the gguf file. Enter genai-os/docker/compose directory and copy the .yaml file. cp llamacpp.yaml llamacpp-<name>.yaml Edit llamacpp-<name>.yaml : line 2: change llamacpp-executor to llamacpp-<name>-executor line 8: change taide-4bit to llamacpp-<name> line 9: change TAIDE 4bit to <access code> , recommended a non-repeated model version name line 10: change Gemini to <the code that will be shown in chat room interface> line 15: change command: [\"--model_path\", \"/var/model/taide-4bit.gguf\" ...... to command: [\"--model_path\", \"/var/model/<gguf file name>\" ...... line 17: change [\"/path/to/taide/model.gguf to your path to the gguf file, and change /var/model/taide-4bit.gguf to /var/model/<gguf file name> Edit run.sh : Add <yaml file name> into confs array and execute run.sh again.","title":"6. Others, using GGUF"},{"location":"docker_quick_installation/","text":"Kuwa Quick Installation for Linux/docker Download the script or the executable file, run it, and follow its steps to have your own Kuwa! Download and run sudo build.sh , or invoke the following command to automatically install Docker, CUDA, and Kuwa. You may need to reboot after installing CUDA. Before finishing installation, you will be asked to set your administration passwords for your Kuwa and database. After installation, it will invoke run.sh to start the system and you can log in with admin@localhost. Enjoy! bash! curl -fsSL https://raw.githubusercontent.com/kuwaai/genai-os/main/docker/build.sh | sudo bash","title":"Docker quick installation"},{"location":"docker_quick_installation/#kuwa-quick-installation-for-linuxdocker","text":"Download the script or the executable file, run it, and follow its steps to have your own Kuwa! Download and run sudo build.sh , or invoke the following command to automatically install Docker, CUDA, and Kuwa. You may need to reboot after installing CUDA. Before finishing installation, you will be asked to set your administration passwords for your Kuwa and database. After installation, it will invoke run.sh to start the system and you can log in with admin@localhost. Enjoy! bash! curl -fsSL https://raw.githubusercontent.com/kuwaai/genai-os/main/docker/build.sh | sudo bash","title":"Kuwa Quick Installation for Linux/docker"},{"location":"faq/","text":"Frequently Asked Questions System Features 1-1. Why is it called Kuwa? A: Kuwa is taken from the Siraya language of the public hall . According to historical records, the public hall had the function of a \"meeting place.\" We hope to create a meeting place for communication between humans and AI models, hence the name Kuwa. Netizens have given it a homophonic name \"Cool! / Cool Frog,\" and in Taiwanese Hokkien, it sounds like \"rely on me.\" 1-2. What are the features of Kuwa GenAI OS? A: Freedom and diversity are the main features of Kuwa: it supports multiple languages, users can chat with different selected models or Bot applications simultaneously, can easily quote messages or specify answers, switch between single-turn or coherent Q&A as needed; the system can run on desktops, laptops, servers, or cloud containers, supporting Windows and Linux; models or applications can be deployed in a distributed manner on-premises or in the cloud, or integrated with external commercial models through APIs; Kuwa supports flexible group permission management, as well as various account creation methods, including invitation codes, authentication API integration, LDAP integration, etc., which can be directly used to provide commercial services. 1-3. Can Kuwa apply for an account like ChatGPT? A: Kuwa currently does not directly provide user account registration or application hosting services. Kuwa GenAI OS opens the entire platform system for everyone to use to build their own testing, development, or service deployment platform. Kuwa has been applied in the TAIDE Exhibition Platform , National University of Kaohsiung Generative AI Service Platform , and other specific application platforms to provide services. 1-4. Can Kuwa GenAI OS operate on-premises or only in the cloud? A: It can do both, as the entire system is open source, users can set up the entire system on-premises or in a private cloud. For detailed installation instructions, please refer to Kuwa GenAI OS's GitHub . 1-5. Which type of users does Kuwa GenAI OS system building lean towards? A: Currently, the system building part leans more towards developers. If you encounter technical issues during the installation process, feel free to contact us for further assistance! 1-6. Does Kuwa GenAI OS have a demonstration system now? A: The Kuwa system has been used in the TAIDE Exhibition Platform , National University of Kaohsiung Generative AI Service Platform , and other specific application platforms to provide services. If you adopt Kuwa GenAI OS, please let us know. 1-7. Can this system be used without a GPU? A: Yes, for the model part, you can connect to the cloud (such as ChatGPT API; Gemini API) or run the model on a GPU (such as NVIDIA CUDA) or CPU (such as LLaMA.cpp) locally. 1-8. Does Kuwa GenAI OS only support on-premises models? A: In addition to on-premises models, it can also integrate cloud models such as OpenAI GPT3.5/4, Google Gemini, etc. 1-9. Is this TAIDE model open source? A: No, the Kuwa system was developed with the support of the Taiwan's Trusted AI Dialogue Engine (TAIDE) project, but it does not currently include the TAIDE model itself. In the future, when the TAIDE model is made public, it can be directly integrated into the Kuwa system. 1-10. What applications does Kuwa GenAI OS support? A: Currently, it supports simple large language model Q&A and RAG. RAG currently supports four types: Search QA, Web QA, Doc QA, and DB QA explained in detail as follows: - Search QA: Q&A using Google Search and web crawling - Doc QA: Q&A based on uploading a single document - Web QA: Q&A for a single webpage - DBQA: Q&A based on a pre-established knowledge base As an open-source system, developers can refer to the source code and develop RAG and applications that better suit their needs. 1-11. What does OS stand for in the name Kuwa GenAI OS? Will it affect my existing OS? A: Since this system manages and allocates underlying model resources to the upper-level GenAI application, similar to a traditional OS or Distributed OS, it is named GenAI OS. GenAI OS is a Web-based system composed of multiple modules with different functions. All modules can run on a single machine, or each module can run on a different OS or hardware. It will not replace your existing OS. Installation and Configuration 2-1. How to install the Kuwa system? A: Please refer to the installation instructions in the README.md file . Feel free to message us if you encounter any problems! Linux instructions Windows instructions 2-2. How to integrate ChatGPT or Gemini? A: Please refer to the instructions in this document . Feel free to contact us if you encounter any problems! 2-3. How to integrate on-premise models? A: Please refer to this tutorial . Feel free to message us if you encounter any problems! 2-4. I have trained my own model, how can I integrate it with Kuwa GenAI OS? A: Please refer to the tutorial for setting up the model, load your model, and prepare the input and output functions to set it up on the system. Documentation 2-5. Can I integrate other inference engines like TGI or vLLM? A: Any inference engine that implements the OpenAI API can be integrated. Other types of APIs are planned for future versions, and we welcome anyone interested to assist in implementation. 2-6. How to add new models? A: Please refer to this tutorial . Feel free to message us if you encounter any problems! Usage 3-1. How to deal with being stuck at \"Message processing... please wait...\"? A: To resolve this issue, please log in to the website management interface using the administrator account and click \"Reset Redis Cache.\" This problem typically occurs when the Model worker process is forcefully closed or unexpectedly exits, which is a known issue that we are working to fix. 3-2. How to use WebQA/DocQA/SearchQA? A: Paste a link to Web QA to ask questions about a single webpage content. Upload a file to Doc QA to ask questions about that file. Just ask your question directly to Search QA to get answers. Community 4-1. How can I get involved? A: Welcome to join our community! Feel free to set up your own and play around. If you feel there is room for improvement, you can directly submit a Pull Request to help us improve! 4-2. What communities does Kuwa have? A: Please refer to the \" Community \" page on our official website.","title":"Frequently Asked Questions"},{"location":"faq/#frequently-asked-questions","text":"","title":"Frequently Asked Questions"},{"location":"faq/#system-features","text":"1-1. Why is it called Kuwa? A: Kuwa is taken from the Siraya language of the public hall . According to historical records, the public hall had the function of a \"meeting place.\" We hope to create a meeting place for communication between humans and AI models, hence the name Kuwa. Netizens have given it a homophonic name \"Cool! / Cool Frog,\" and in Taiwanese Hokkien, it sounds like \"rely on me.\" 1-2. What are the features of Kuwa GenAI OS? A: Freedom and diversity are the main features of Kuwa: it supports multiple languages, users can chat with different selected models or Bot applications simultaneously, can easily quote messages or specify answers, switch between single-turn or coherent Q&A as needed; the system can run on desktops, laptops, servers, or cloud containers, supporting Windows and Linux; models or applications can be deployed in a distributed manner on-premises or in the cloud, or integrated with external commercial models through APIs; Kuwa supports flexible group permission management, as well as various account creation methods, including invitation codes, authentication API integration, LDAP integration, etc., which can be directly used to provide commercial services. 1-3. Can Kuwa apply for an account like ChatGPT? A: Kuwa currently does not directly provide user account registration or application hosting services. Kuwa GenAI OS opens the entire platform system for everyone to use to build their own testing, development, or service deployment platform. Kuwa has been applied in the TAIDE Exhibition Platform , National University of Kaohsiung Generative AI Service Platform , and other specific application platforms to provide services. 1-4. Can Kuwa GenAI OS operate on-premises or only in the cloud? A: It can do both, as the entire system is open source, users can set up the entire system on-premises or in a private cloud. For detailed installation instructions, please refer to Kuwa GenAI OS's GitHub . 1-5. Which type of users does Kuwa GenAI OS system building lean towards? A: Currently, the system building part leans more towards developers. If you encounter technical issues during the installation process, feel free to contact us for further assistance! 1-6. Does Kuwa GenAI OS have a demonstration system now? A: The Kuwa system has been used in the TAIDE Exhibition Platform , National University of Kaohsiung Generative AI Service Platform , and other specific application platforms to provide services. If you adopt Kuwa GenAI OS, please let us know. 1-7. Can this system be used without a GPU? A: Yes, for the model part, you can connect to the cloud (such as ChatGPT API; Gemini API) or run the model on a GPU (such as NVIDIA CUDA) or CPU (such as LLaMA.cpp) locally. 1-8. Does Kuwa GenAI OS only support on-premises models? A: In addition to on-premises models, it can also integrate cloud models such as OpenAI GPT3.5/4, Google Gemini, etc. 1-9. Is this TAIDE model open source? A: No, the Kuwa system was developed with the support of the Taiwan's Trusted AI Dialogue Engine (TAIDE) project, but it does not currently include the TAIDE model itself. In the future, when the TAIDE model is made public, it can be directly integrated into the Kuwa system. 1-10. What applications does Kuwa GenAI OS support? A: Currently, it supports simple large language model Q&A and RAG. RAG currently supports four types: Search QA, Web QA, Doc QA, and DB QA explained in detail as follows: - Search QA: Q&A using Google Search and web crawling - Doc QA: Q&A based on uploading a single document - Web QA: Q&A for a single webpage - DBQA: Q&A based on a pre-established knowledge base As an open-source system, developers can refer to the source code and develop RAG and applications that better suit their needs. 1-11. What does OS stand for in the name Kuwa GenAI OS? Will it affect my existing OS? A: Since this system manages and allocates underlying model resources to the upper-level GenAI application, similar to a traditional OS or Distributed OS, it is named GenAI OS. GenAI OS is a Web-based system composed of multiple modules with different functions. All modules can run on a single machine, or each module can run on a different OS or hardware. It will not replace your existing OS.","title":"System Features"},{"location":"faq/#installation-and-configuration","text":"2-1. How to install the Kuwa system? A: Please refer to the installation instructions in the README.md file . Feel free to message us if you encounter any problems! Linux instructions Windows instructions 2-2. How to integrate ChatGPT or Gemini? A: Please refer to the instructions in this document . Feel free to contact us if you encounter any problems! 2-3. How to integrate on-premise models? A: Please refer to this tutorial . Feel free to message us if you encounter any problems! 2-4. I have trained my own model, how can I integrate it with Kuwa GenAI OS? A: Please refer to the tutorial for setting up the model, load your model, and prepare the input and output functions to set it up on the system. Documentation 2-5. Can I integrate other inference engines like TGI or vLLM? A: Any inference engine that implements the OpenAI API can be integrated. Other types of APIs are planned for future versions, and we welcome anyone interested to assist in implementation. 2-6. How to add new models? A: Please refer to this tutorial . Feel free to message us if you encounter any problems!","title":"Installation and Configuration"},{"location":"faq/#usage","text":"3-1. How to deal with being stuck at \"Message processing... please wait...\"? A: To resolve this issue, please log in to the website management interface using the administrator account and click \"Reset Redis Cache.\" This problem typically occurs when the Model worker process is forcefully closed or unexpectedly exits, which is a known issue that we are working to fix. 3-2. How to use WebQA/DocQA/SearchQA? A: Paste a link to Web QA to ask questions about a single webpage content. Upload a file to Doc QA to ask questions about that file. Just ask your question directly to Search QA to get answers.","title":"Usage"},{"location":"faq/#community","text":"4-1. How can I get involved? A: Welcome to join our community! Feel free to set up your own and play around. If you feel there is room for improvement, you can directly submit a Pull Request to help us improve! 4-2. What communities does Kuwa have? A: Please refer to the \" Community \" page on our official website.","title":"Community"},{"location":"legal_information/","text":"Legal Information License of the source code: MIT License of the documentation: CC BY-SA 4.0","title":"Legal Information"},{"location":"legal_information/#legal-information","text":"License of the source code: MIT License of the documentation: CC BY-SA 4.0","title":"Legal Information"},{"location":"troubleshooting/","text":"Troubleshooting If you encounter any technical issues, have a look at our FAQs to see if your problem is addressed there. If you still cannot resolve your issue, feel free to ask our community and we will do our best to help you troubleshoot any issues. When asking for help, please include information about your system's hardware, software versions, and what the expected and actual behavior is, as this will help us assist you in troubleshooting more quickly.","title":"Troubleshooting"},{"location":"troubleshooting/#troubleshooting","text":"If you encounter any technical issues, have a look at our FAQs to see if your problem is addressed there. If you still cannot resolve your issue, feel free to ask our community and we will do our best to help you troubleshoot any issues. When asking for help, please include information about your system's hardware, software versions, and what the expected and actual behavior is, as this will help us assist you in troubleshooting more quickly.","title":"Troubleshooting"},{"location":"windows_installation/","text":"Portable Installation Guide for Windows We provide a portable version for Windows x64, which uses SQLite as the database by default. After unzipping, the required packages will take up about 1.5GB. Please pay attention to your data usage. Please follow the steps below to install: Prerequisites Make sure you have already installed VC_redist.exe If you want to load models on GPU, please install CUDA first. Quick Installation Gemini and ChatGPT are enabled by default. If you need to run the gguf model, please drop it in the path: executors\\llamacpp. The process will create an admin account. If you need a new one, please refer to the first item in FAQs . git clone https://github.com/kuwaai/genai-os.git cd genai-os/windows \"build & start.bat\" Enter the stop command to close the system. Directly closing the window may fail to release memory usage. If you accidentally closed the window, please refer to the third item in FAQs . You can directly run start.bat for subsequent startup. If there is an update or the project path is moved, please rerun build.bat or build & start.bat . Detailed Installation Steps Download from Release, or execute the following command in git bash to clone the project and switch to the windows folder in the project: bat git clone https://github.com/kuwaai/genai-os.git cd genai-os/windows Download the related packages and set up quickly: bat .\\build.bat Start the application: Run start.bat to start the application. Note: If you have any of the following services running (nginx, php, php-cgi, python, redis-server), this executable will terminate them when closed. Please also make sure that ports 80, 9000, and 6379 are not being used. bat .\\start.bat At this point, you should be asked to create an administrator account (you will need to enter a name, email address, and password). If it does not pop up or you enter it incorrectly or fail to create it, please see here . Check the application status: If successful, your browser will automatically open to 127.0.0.1 . If you see the web interface, the installation should be successful. How to close the program: Please try not to force close the .bat file (including using the red cross to close it directly). Currently, due to the .bat file, it cannot automatically close all open programs to release resources in these situations. Therefore, please develop the habit of entering stop when executing start.bat to close the program. Set up models: By default, ChatGPT and Gemini are preset when the program is just started. Both models are connected to the API, so you need to apply for the corresponding API Key. If you want to start your own model or connect to other APIs, you need to set up executors. However, since this part is extensive, please refer to the tutorial guide here . FAQ Q: I was not asked to create an administrator account, the administrator account creation failed, or I entered it incorrectly... A: Please open tool.bat , then enter seed to open the administrator account creation interface. Enter quit to close after creation. Q: After moving the entire project, I got a bunch of errors when executing start.bat, and the webpage was 404/500 and could not be accessed. A: Since some parts of the project must use absolute paths, if the path to the project directory has changed (a change in the name of the parent folder or the moving of the entire project), you will need to rerun build.bat to update the absolute path, and so does the model in the workers folder. It is recommended to rerun init.bat to avoid errors. Q: I accidentally closed the entire start.bat program by directly clicking the red cross; the background program was not closed, and the memory resources are still occupied. What should I do? A: Due to the .bat file, it cannot close all programs when you click the red cross. You can open tool.bat and enter stop to terminate all related programs. Please feel free to contact us if you encounter any problems during the installation.","title":"Windows installation"},{"location":"windows_installation/#portable-installation-guide-for-windows","text":"We provide a portable version for Windows x64, which uses SQLite as the database by default. After unzipping, the required packages will take up about 1.5GB. Please pay attention to your data usage. Please follow the steps below to install:","title":"Portable Installation Guide for Windows"},{"location":"windows_installation/#prerequisites","text":"Make sure you have already installed VC_redist.exe If you want to load models on GPU, please install CUDA first.","title":"Prerequisites"},{"location":"windows_installation/#quick-installation","text":"Gemini and ChatGPT are enabled by default. If you need to run the gguf model, please drop it in the path: executors\\llamacpp. The process will create an admin account. If you need a new one, please refer to the first item in FAQs . git clone https://github.com/kuwaai/genai-os.git cd genai-os/windows \"build & start.bat\" Enter the stop command to close the system. Directly closing the window may fail to release memory usage. If you accidentally closed the window, please refer to the third item in FAQs . You can directly run start.bat for subsequent startup. If there is an update or the project path is moved, please rerun build.bat or build & start.bat .","title":"Quick Installation"},{"location":"windows_installation/#detailed-installation-steps","text":"Download from Release, or execute the following command in git bash to clone the project and switch to the windows folder in the project: bat git clone https://github.com/kuwaai/genai-os.git cd genai-os/windows Download the related packages and set up quickly: bat .\\build.bat Start the application: Run start.bat to start the application. Note: If you have any of the following services running (nginx, php, php-cgi, python, redis-server), this executable will terminate them when closed. Please also make sure that ports 80, 9000, and 6379 are not being used. bat .\\start.bat At this point, you should be asked to create an administrator account (you will need to enter a name, email address, and password). If it does not pop up or you enter it incorrectly or fail to create it, please see here . Check the application status: If successful, your browser will automatically open to 127.0.0.1 . If you see the web interface, the installation should be successful. How to close the program: Please try not to force close the .bat file (including using the red cross to close it directly). Currently, due to the .bat file, it cannot automatically close all open programs to release resources in these situations. Therefore, please develop the habit of entering stop when executing start.bat to close the program. Set up models: By default, ChatGPT and Gemini are preset when the program is just started. Both models are connected to the API, so you need to apply for the corresponding API Key. If you want to start your own model or connect to other APIs, you need to set up executors. However, since this part is extensive, please refer to the tutorial guide here .","title":"Detailed Installation Steps"},{"location":"windows_installation/#faq","text":"Q: I was not asked to create an administrator account, the administrator account creation failed, or I entered it incorrectly... A: Please open tool.bat , then enter seed to open the administrator account creation interface. Enter quit to close after creation. Q: After moving the entire project, I got a bunch of errors when executing start.bat, and the webpage was 404/500 and could not be accessed. A: Since some parts of the project must use absolute paths, if the path to the project directory has changed (a change in the name of the parent folder or the moving of the entire project), you will need to rerun build.bat to update the absolute path, and so does the model in the workers folder. It is recommended to rerun init.bat to avoid errors. Q: I accidentally closed the entire start.bat program by directly clicking the red cross; the background program was not closed, and the memory resources are still occupied. What should I do? A: Due to the .bat file, it cannot close all programs when you click the red cross. You can open tool.bat and enter stop to terminate all related programs. Please feel free to contact us if you encounter any problems during the installation.","title":"FAQ"},{"location":"windows_model_setup/","text":"Windows Portable Model Deployment Tutorial Currently, for simplifying model deployment in Windows, a simple model management system is prepared in the Windows portable version. This management system is only for simple use and testing. It is not recommended to be used in Production scenarios. If you want to use it in Production, please refer to the tutorial . This model deployment tutorial assumes that you have executed windows/build & start.bat or windows/build.bat + windows/start.bat , and the system can log in without a problem. If the above is not your case, please go back to the steps of this tutorial . Introduction In the windows/executors folder, you should see the following folders. You can change the folder name at your will; however, the following five names are reserved by default for easy model setup: 1. chatgpt 2. custom 3. geminipro 4. huggingface 5. llamacpp Each folder has an init.bat file to configure the run.bat file. You can edit the run.bat file directly or write your own file; however, make sure the parameters and format are correct. After the configuration is complete, restart the start.bat file to take effect. You can also place images in the folder to automatically put them into the model when the model is first created (if the model has already been created on the website, you need to manually put the images in). By default, these models will only be available to users with Manage Tab permissions. Quick Model Setup Tutorial ChatGPT Enter the chatgpt folder. Execute the init.bat file. Enter the OpenAI API Token. If you do not want to set a global token, you can leave it blank and press Enter to skip. Enter additional parameters (please refer to the tutorial for the kuwa-executor command parameters. If you are not sure what to fill in, leave it blank and press Enter to skip) Gemini Enter the geminipro folder. Execute the init.bat file. Enter the Google API Token. If you do not want to set a global token, you can leave it blank and press Enter to skip. Enter additional parameters (please refer to the tutorial for the kuwa-executor command parameters. If you are not sure what to fill in, leave it blank and press Enter to skip) LLaMA.cpp Enter the llamacpp folder. Put the .gguf file in the folder. Execute the init.bat file. If prompted for a missing .gguf file, enter the absolute path to the file. Enter additional parameters (please refer to the tutorial for the kuwa-executor command parameters. If you are not sure what to fill in, leave it blank and press Enter directly) Huggingface Enter the huggingface folder. Put the model and tokenizer. Execute the init.bat file, and it will automatically detect whether there is a model in the directory. If it is not automatically detected, enter the absolute path to the model folder or the location of the model on huggingface. Enter additional parameters (please refer to the tutorial for the kuwa-executor command parameters. If you are not sure what to fill in, leave it blank and press Enter) Custom This is a reserved custom model. Users can rewrite a version of the executor (inheriting kuwa LLMWorker), and specify the .py file here for easy execution. The steps are the same as above, but there is an additional worker_path parameter, which needs to be used to point to the absolute path of the .py file. Advanced Usage You can execute multiple models in multiple folders using the same access_code . This allows you to handle multiple requests simultaneously. You can also duplicate folders to create more model executions. These models do not have to be on the same host; you can distribute them across multiple hosts. Just set the Kernel endpoint to the kernel. For detailed instructions, please see here . If you are using other TGI frameworks like ollama or vLLM, you can also quickly concatenate using ChatGPT's worker.","title":"Windows model setup"},{"location":"windows_model_setup/#windows-portable-model-deployment-tutorial","text":"Currently, for simplifying model deployment in Windows, a simple model management system is prepared in the Windows portable version. This management system is only for simple use and testing. It is not recommended to be used in Production scenarios. If you want to use it in Production, please refer to the tutorial . This model deployment tutorial assumes that you have executed windows/build & start.bat or windows/build.bat + windows/start.bat , and the system can log in without a problem. If the above is not your case, please go back to the steps of this tutorial .","title":"Windows Portable Model Deployment Tutorial"},{"location":"windows_model_setup/#introduction","text":"In the windows/executors folder, you should see the following folders. You can change the folder name at your will; however, the following five names are reserved by default for easy model setup: 1. chatgpt 2. custom 3. geminipro 4. huggingface 5. llamacpp Each folder has an init.bat file to configure the run.bat file. You can edit the run.bat file directly or write your own file; however, make sure the parameters and format are correct. After the configuration is complete, restart the start.bat file to take effect. You can also place images in the folder to automatically put them into the model when the model is first created (if the model has already been created on the website, you need to manually put the images in). By default, these models will only be available to users with Manage Tab permissions.","title":"Introduction"},{"location":"windows_model_setup/#quick-model-setup-tutorial","text":"","title":"Quick Model Setup Tutorial"},{"location":"windows_model_setup/#chatgpt","text":"Enter the chatgpt folder. Execute the init.bat file. Enter the OpenAI API Token. If you do not want to set a global token, you can leave it blank and press Enter to skip. Enter additional parameters (please refer to the tutorial for the kuwa-executor command parameters. If you are not sure what to fill in, leave it blank and press Enter to skip)","title":"ChatGPT"},{"location":"windows_model_setup/#gemini","text":"Enter the geminipro folder. Execute the init.bat file. Enter the Google API Token. If you do not want to set a global token, you can leave it blank and press Enter to skip. Enter additional parameters (please refer to the tutorial for the kuwa-executor command parameters. If you are not sure what to fill in, leave it blank and press Enter to skip)","title":"Gemini"},{"location":"windows_model_setup/#llamacpp","text":"Enter the llamacpp folder. Put the .gguf file in the folder. Execute the init.bat file. If prompted for a missing .gguf file, enter the absolute path to the file. Enter additional parameters (please refer to the tutorial for the kuwa-executor command parameters. If you are not sure what to fill in, leave it blank and press Enter directly)","title":"LLaMA.cpp"},{"location":"windows_model_setup/#huggingface","text":"Enter the huggingface folder. Put the model and tokenizer. Execute the init.bat file, and it will automatically detect whether there is a model in the directory. If it is not automatically detected, enter the absolute path to the model folder or the location of the model on huggingface. Enter additional parameters (please refer to the tutorial for the kuwa-executor command parameters. If you are not sure what to fill in, leave it blank and press Enter)","title":"Huggingface"},{"location":"windows_model_setup/#custom","text":"This is a reserved custom model. Users can rewrite a version of the executor (inheriting kuwa LLMWorker), and specify the .py file here for easy execution. The steps are the same as above, but there is an additional worker_path parameter, which needs to be used to point to the absolute path of the .py file.","title":"Custom"},{"location":"windows_model_setup/#advanced-usage","text":"You can execute multiple models in multiple folders using the same access_code . This allows you to handle multiple requests simultaneously. You can also duplicate folders to create more model executions. These models do not have to be on the same host; you can distribute them across multiple hosts. Just set the Kernel endpoint to the kernel. For detailed instructions, please see here . If you are using other TGI frameworks like ollama or vLLM, you can also quickly concatenate using ChatGPT's worker.","title":"Advanced Usage"},{"location":"release/2024-04-08-first-stable-release-v0.1.0/","text":"Hello developers and users, After receiving feedback from many users since the initial release, we are pleased to announce the stable release of v0.1.0 . In this version, we have made some adjustments to the installation process for the Windows version. We have also simultaneously released a Docker version, allowing users to quickly install and adjust the environment structure as needed. Additionally, we have fixed some minor bugs that were known in previous versions. Here are the main updates in this release: Windows Portable Version Adjusted the model setup process to allow for easier configuration of multiple models. Fixed various errors that occurred when using MySQL or PostgreSQL. Readme updated for better completeness. Docker Version Docker Compose can now be used to start the entire system and multiple Executors with a single command. Stable software stack selected, suitable for direct use in production environments. Modular design allows for the selection of Executor types and quantities to be launched freely. Executor Added a command-line interface launcher that can start multiple Executors with one click, allowing common parameters such as Prompt template, System Prompt, and Generation config to be passed in as commands. Supports common on-premises model inference frameworks such as Huggingface Transformers and Llama.cpp. Supports inference services compatible with OpenAI API or Gemini-Pro API, such as vLLM, LiteLLM, etc. Packaged common functions into the Executor framework, such as automatic registration retry, automatic logout, automatic history record pruning, interrupt generation, etc. Packaged the Executor framework into a package for easy extension of Executors. Fixed a bug in the generation error of the llama.cpp executor. Changed the underlying framework to FastAPI to improve efficiency and stability. Multi-chat Fixed bug causing website to jump to /stream route. Added default images for models. Fixed some minor bugs. Added more command-line tools for configuring the website. For migration from older versions to the new version, please refer to this migration guide .","title":"2024 04 08 first stable release v0.1.0"},{"location":"release/2024-04-08-first-stable-release-v0.1.0/#windows-portable-version","text":"Adjusted the model setup process to allow for easier configuration of multiple models. Fixed various errors that occurred when using MySQL or PostgreSQL. Readme updated for better completeness.","title":"Windows Portable Version"},{"location":"release/2024-04-08-first-stable-release-v0.1.0/#docker-version","text":"Docker Compose can now be used to start the entire system and multiple Executors with a single command. Stable software stack selected, suitable for direct use in production environments. Modular design allows for the selection of Executor types and quantities to be launched freely.","title":"Docker Version"},{"location":"release/2024-04-08-first-stable-release-v0.1.0/#executor","text":"Added a command-line interface launcher that can start multiple Executors with one click, allowing common parameters such as Prompt template, System Prompt, and Generation config to be passed in as commands. Supports common on-premises model inference frameworks such as Huggingface Transformers and Llama.cpp. Supports inference services compatible with OpenAI API or Gemini-Pro API, such as vLLM, LiteLLM, etc. Packaged common functions into the Executor framework, such as automatic registration retry, automatic logout, automatic history record pruning, interrupt generation, etc. Packaged the Executor framework into a package for easy extension of Executors. Fixed a bug in the generation error of the llama.cpp executor. Changed the underlying framework to FastAPI to improve efficiency and stability.","title":"Executor"},{"location":"release/2024-04-08-first-stable-release-v0.1.0/#multi-chat","text":"Fixed bug causing website to jump to /stream route. Added default images for models. Fixed some minor bugs. Added more command-line tools for configuring the website. For migration from older versions to the new version, please refer to this migration guide .","title":"Multi-chat"},{"location":"release/2024-04-08-migration-from-old-to-v0.1.0/","text":"This is a tutorial for updating from the initial version to the stable version v0.1.0. First, clone the repository using git clone https://github.com/kuwaai/genai-os.git --tag v0.1.0 , or download and extract it from here to get a clean copy of the v0.1.0 project. Here, the old version of the project is referred to as the old folder, and the newly obtained version is referred to as the new folder. If you have these files, please copy them completely and replace them in the corresponding locations: old/multi-chat/storage/app/ => new/src/multi-chat/storage/app/ old/multi-chat/database/database.sqlite => new/src/multi-chat/database/database.sqlite old/multi-chat/public => new/src/multi-chat/public old/multi-chat/.env => new/src/multi-chat/.env In addition to these files mentioned in point two, if you have modified or added any other files, please copy them over as well. If you are using the Windows portable version, please move the following folders or files to their respective locations (since the Python version has changed, there is no need to move the Python folder): old/windows/nginx-1.24.0/ => new/windows/packages/nginx-1.24.0/ old/windows/node-v20.11.1-win-x64/ => new/windows/packages/node-v20.11.1-win-x64/ old/windows/php-8.1.27-Win32-vs16-x64/ => new/windows/packages/php-8.1.27-Win32-vs16-x64/ old/windows/Redis-6.0.20-Windows-x64-msys2/ => new/windows/packages/Redis-6.0.20-Windows-x64-msys2/ old/windows/RunHiddenConsole/ => new/windows/packages/RunHiddenConsole/ old/windows/composer.phar => new/windows/packages/composer.phar If you are running on Linux, navigate to new/src/multi-chat/executables/sh/ and run production_update.sh . If you are using the Windows Portable version, run build.bat in new/windows/ . The file update should be completed at this point. You can now check if anything is broken. For the Windows Portable version, please proceed to configure the models according to the tutorial for the new version .","title":"2024 04 08 migration from old to v0.1.0"},{"location":"release/2024-04-15-release-v0.2.0-beta/","text":"[!Note] This version does not include the TAIDE model itself, and a version pre-loaded with the TAIDE model is expected to be released after the TAIDE model is publicly available. Hello to our community friends, After collecting everyone's feedback, we plan to roll out the long-awaited RAG feature in v0.2.0. The RAG part has been internally tested, so we are releasing v0.2.0-beta to invite everyone to test it out and see if it meets your expectations. In addition, this update also provides a way to connect with TAIDE API and TAIDE models. At the same time, we have also adjusted the system installation script and fixed some known bugs, making the entire system more stable, easier to extend, and easier to use. If you have any suggestions or if you think there is room for improvement, please let us know! The details of this update are as follows: Windows Portable Edition Adjust the model hosting method: Enable Gemini and ChatGPT APIs by default Use Gemini by default to launch RAG applications for WebQA and Document QA Deprecate env.bat and use run.bat to launch the executor instead Enhance executor functionality: Allow direct configuration of execution instructions, parameters, and other information Adjust init.bat to be a simple tool to help create run.bat. Users can also directly write run.bat to launch the required model Fix the error of a non-existent PHP download link in v0.1.0 (archived due to version update) Integrate RAG into the simplified launch framework of the Windows version Specify file path improvements: In the executors folder of the Windows version, files will be specified using relative paths by default Fix the executor error of the Custom category Permission adjustment: Only groups with Manage Tab permission can be directly assigned permission to use the model when the model is added Fix the issue in the Windows version where Redis uses localhost as the IP, causing DNS queries to be delayed by 2 seconds each time Docker Edition Integrate RAG (Document QA / Web QA / DB QA) into the executor's Docker image and compose the system Provide a compose example of Gemini using a global API key Complete the missing words in the document Executor Provide a TAIDE API executor, which can be directly connected to the TAIDE API of TECO Port RAG executor (Document QA / Web QA / DB QA / Search QA) to the new framework Let the RAG executor support automatic model search, i18n, and interrupt generation Kernel Provide an API to list currently available executors Multi-chat Adjust the timeout waiting time for the no-message state: Extend from 1 minute to 2 minutes to accommodate the waiting time for the RAG processing speed gap Fix the bug of AdminSeeder: Fix the issue of granting duplicate model usage permissions Add the function of sending Kuwa tokens from the website to the executor: To fix the past issue of unowned API tokens for RAG Add a method to adjust the default model image path: Can be configured via LLM_DEFAULT_IMG in the .env file Fix the bug that the API in v0.1.0 could not be used normally Known Issues and Limitations At present, the Windows version of Document QA can process files in .doc and .docx formats. However, due to library dependency issues, it may not be able to read certain .pdf files. If such a need arises, please consider utilizing the Linux version of Kuwa for PDF processing. RAG applications tend to generate long input. If only using the CPU-based version of the on-premises model, timeout errors can occur more easily. In this case, we recommend either using the cloud-based model, or using the GPU-based version of the on-premises model, and then using the RAG application.","title":"2024 04 15 release v0.2.0 beta"},{"location":"release/2024-04-15-release-v0.2.0-beta/#windows-portable-edition","text":"Adjust the model hosting method: Enable Gemini and ChatGPT APIs by default Use Gemini by default to launch RAG applications for WebQA and Document QA Deprecate env.bat and use run.bat to launch the executor instead Enhance executor functionality: Allow direct configuration of execution instructions, parameters, and other information Adjust init.bat to be a simple tool to help create run.bat. Users can also directly write run.bat to launch the required model Fix the error of a non-existent PHP download link in v0.1.0 (archived due to version update) Integrate RAG into the simplified launch framework of the Windows version Specify file path improvements: In the executors folder of the Windows version, files will be specified using relative paths by default Fix the executor error of the Custom category Permission adjustment: Only groups with Manage Tab permission can be directly assigned permission to use the model when the model is added Fix the issue in the Windows version where Redis uses localhost as the IP, causing DNS queries to be delayed by 2 seconds each time","title":"Windows Portable Edition"},{"location":"release/2024-04-15-release-v0.2.0-beta/#docker-edition","text":"Integrate RAG (Document QA / Web QA / DB QA) into the executor's Docker image and compose the system Provide a compose example of Gemini using a global API key Complete the missing words in the document","title":"Docker Edition"},{"location":"release/2024-04-15-release-v0.2.0-beta/#executor","text":"Provide a TAIDE API executor, which can be directly connected to the TAIDE API of TECO Port RAG executor (Document QA / Web QA / DB QA / Search QA) to the new framework Let the RAG executor support automatic model search, i18n, and interrupt generation","title":"Executor"},{"location":"release/2024-04-15-release-v0.2.0-beta/#kernel","text":"Provide an API to list currently available executors","title":"Kernel"},{"location":"release/2024-04-15-release-v0.2.0-beta/#multi-chat","text":"Adjust the timeout waiting time for the no-message state: Extend from 1 minute to 2 minutes to accommodate the waiting time for the RAG processing speed gap Fix the bug of AdminSeeder: Fix the issue of granting duplicate model usage permissions Add the function of sending Kuwa tokens from the website to the executor: To fix the past issue of unowned API tokens for RAG Add a method to adjust the default model image path: Can be configured via LLM_DEFAULT_IMG in the .env file Fix the bug that the API in v0.1.0 could not be used normally","title":"Multi-chat"},{"location":"release/2024-04-15-release-v0.2.0-beta/#known-issues-and-limitations","text":"At present, the Windows version of Document QA can process files in .doc and .docx formats. However, due to library dependency issues, it may not be able to read certain .pdf files. If such a need arises, please consider utilizing the Linux version of Kuwa for PDF processing. RAG applications tend to generate long input. If only using the CPU-based version of the on-premises model, timeout errors can occur more easily. In this case, we recommend either using the cloud-based model, or using the GPU-based version of the on-premises model, and then using the RAG application.","title":"Known Issues and Limitations"},{"location":"release/2024-04-15-release-v0.2.0-taide/","text":"Hello friends, The TAIDE model was released today, and we are happy to release a customized Kuwa system for Windows that has the built-in TAIDE LX 7B Chat 4bit model. [!Note] Download link for the kuwa-taide-v0.2.0 single executable file: Google Drive kuwa-taide-v0.2.0 documentation: kuwa-taide-0415.pdf This customized system is a self-extracting single executable file, and the TAIDE model is built-in as the default local model option, and it can be run in both CPU and GPU environments, allowing everyone to quickly and easily experience the effects of the TAIDE model and perform GenAI-related applications. In addition, this system is customized based on the previously released v0.2.0-beta, so you can also use the TAIDE model for RAG applications. However, it is important to note that RAG will generate longer input, so it is recommended to use the GPU version for inference. The Kuwa system and TAIDE model are still under development and improvement, so it is inevitable that they will be unstable. The content generated by this system is for reference only and does not guarantee its correctness. Users still need to verify it; please do not publish inappropriate dialogue content to avoid unexpected problems. TAIDE official website: https://taide.tw/ Kuwa official website: https://kuwaai.tw/zh-Hant/","title":"2024 04 15 release v0.2.0 taide"},{"location":"release/2024-04-29-migrate-to-v0.2.1/","text":"This post will guide you to upgrade Kuwa TAIDE\u2019s built-in model from TAIDE-LX-7B-Chat-4bit to Llama3-TAIDE-LX-8B-Chat-Alpha1-4bit. Go to C:\\kuwa\\GenAI OS\\windows\\executors , and duplicate the 1_taide directory to 1_taide-8b . If you only need to run the new version of TAIDE model, you can delete the run.bat file in 1_taide . Download taide-8b-a.3-q4_k_m.gguf from the official TAIDE HuggingFace Hub to C:\\kuwa\\GenAI OS\\windows\\executors\\1-taide_8b , and delete the original taide-7b-a.2-q4_k_m.gguf Run init.bat, and use the following settings: Enter the option number (1-5): 3 Enter the model name: Llama3 TAIDE LX 8B Chat Alpha1 4bit Enter the access code: taide-8b Arguments to use (...): --stop \"<|eot_id|>\" Restart Kuwa GenAI OS and you should see the new version of TAIDE model You can use the multi-chat feature to compare the responses from two TAIDE models at the same time","title":"2024 04 29 migrate to v0.2.1"},{"location":"release/2024-04-29-release-v0.2.1-taide/","text":"Kuwa TAIDE v0.2.1 is Released Hello everyone, The TAIDE model released the Llama3-TAIDE-LX-8B-Chat-Alpha1 version today. Friends who use the Kuwa TAIDE version only need to update to the latest v0.2.1 version to experience the latest version of the TAIDE model. In addition to updating the TAIDE model, this version also expands the support for local models and fixes some minor problems, hoping to provide everyone with a better user experience. [!Note] Download link for kuwa-taide-v0.2.1 single executable file: Google Drive The detailed update content is as follows: 1. Integrate GPU and CPU versions 2. Fix the line break bug when exporting chat records 3. Fix the text display error of the copy button 4. Fix the chat record disappearing bug caused by incorrect group chat sorting 5. HuggingFace executor adds support for various new on-premise models recently launched by vendors such as Apple and Microsoft Manual model update tutorial: https://kuwaai.tw/blog/migrate-to-kuwa-os-v0.2.1-taide Llama3-TAIDE-LX-8B-Chat-Alpha1 release post: TAIDE Official Facebook Page Post Kuwa official website: https://kuwaai.tw/","title":"Kuwa TAIDE v0.2.1 is Released"},{"location":"release/2024-04-29-release-v0.2.1-taide/#kuwa-taide-v021-is-released","text":"Hello everyone, The TAIDE model released the Llama3-TAIDE-LX-8B-Chat-Alpha1 version today. Friends who use the Kuwa TAIDE version only need to update to the latest v0.2.1 version to experience the latest version of the TAIDE model. In addition to updating the TAIDE model, this version also expands the support for local models and fixes some minor problems, hoping to provide everyone with a better user experience. [!Note] Download link for kuwa-taide-v0.2.1 single executable file: Google Drive The detailed update content is as follows: 1. Integrate GPU and CPU versions 2. Fix the line break bug when exporting chat records 3. Fix the text display error of the copy button 4. Fix the chat record disappearing bug caused by incorrect group chat sorting 5. HuggingFace executor adds support for various new on-premise models recently launched by vendors such as Apple and Microsoft Manual model update tutorial: https://kuwaai.tw/blog/migrate-to-kuwa-os-v0.2.1-taide Llama3-TAIDE-LX-8B-Chat-Alpha1 release post: TAIDE Official Facebook Page Post Kuwa official website: https://kuwaai.tw/","title":"Kuwa TAIDE v0.2.1 is Released"},{"location":"release/2024-05-07-release-v0.3.0-beta1/","text":"Kuwa TAIDE v0.3.0-beta1 is Released Hello, friends of the community, As planned, we have launched the kuwa-v0.3.0-beta1 version for you to experience in advance \ud83d\ude01 [!Note] kuwa-v0.3.0-beta1 download info: https://github.com/kuwaai/genai-os/releases/tag/v0.3.0-beta1 kuwa-taide-v0.3.0-beta1 single executable download link: https://github.com/kuwaai/genai-os/releases/download/v0.3.0-beta1/kuwa-taide-v0.3.0-beta1.exe This version mainly adds new functions such as Bot, Store, RAG toolchain, as well as new integrated chat and group chat interface: 1. Bot allows users to create Bot applications without code, and the System prompt can be adjusted to achieve different functions such as role-playing and performing specific tasks. The model supports partial Ollama model file settings; 2. Store allows users to independently build and maintain a shared Bot application store, and users can also share the Bot; 3. RAG toolchain allows users to create vector databases of local documents, and then perform Q&A through the existing DBQA function; 4. The new integrated interface not only directly supports group chat and single-model chat, but also allows you to import Prompt Sets or upload files at any time, and can also be used for related RAGs. Welcome to experience it and give us feedback. If you encounter any difficulties, please feel free to contact us \ud83d\ude01 Kuwa official website: https://kuwaai.tw/","title":"Kuwa TAIDE v0.3.0-beta1 is Released"},{"location":"release/2024-05-07-release-v0.3.0-beta1/#kuwa-taide-v030-beta1-is-released","text":"Hello, friends of the community, As planned, we have launched the kuwa-v0.3.0-beta1 version for you to experience in advance \ud83d\ude01 [!Note] kuwa-v0.3.0-beta1 download info: https://github.com/kuwaai/genai-os/releases/tag/v0.3.0-beta1 kuwa-taide-v0.3.0-beta1 single executable download link: https://github.com/kuwaai/genai-os/releases/download/v0.3.0-beta1/kuwa-taide-v0.3.0-beta1.exe This version mainly adds new functions such as Bot, Store, RAG toolchain, as well as new integrated chat and group chat interface: 1. Bot allows users to create Bot applications without code, and the System prompt can be adjusted to achieve different functions such as role-playing and performing specific tasks. The model supports partial Ollama model file settings; 2. Store allows users to independently build and maintain a shared Bot application store, and users can also share the Bot; 3. RAG toolchain allows users to create vector databases of local documents, and then perform Q&A through the existing DBQA function; 4. The new integrated interface not only directly supports group chat and single-model chat, but also allows you to import Prompt Sets or upload files at any time, and can also be used for related RAGs. Welcome to experience it and give us feedback. If you encounter any difficulties, please feel free to contact us \ud83d\ude01 Kuwa official website: https://kuwaai.tw/","title":"Kuwa TAIDE v0.3.0-beta1 is Released"},{"location":"release/2024-05-19-release-v0.3.0/","text":"Hello everyone, after receiving feedback and suggestions from the community, we have launched the official version of kuwa-v0.3.0 to better meet your needs. The main differences from the previous version, kuwa-v0.2.1, are the addition and enhancement of features such as Bot, Store, RAG toolchain, and system updates, as well as a new chat and group chat integration interface: Bot allows users to create Bot applications with no code, and can adjust System prompt, preset chat records, User prompt prefixes and suffixes to implement different functions such as role playing, executing specific tasks, or using Ollama model files to build more powerful applications; Store allows users to build and maintain their own shared Bot application store, and users can also share Bots; RAG toolchain allows users to create their own vector databases by simply dragging and dropping local file folders, and then use the existing DBQA function to perform Q&A; The new integrated interface not only directly supports group chats and single-model chats, it can also import Prompt Sets or upload files at any time, and can also be used for related RAGs; Windows version adds SearchQA, which can be used to organize website Q&A by connecting to Google search; Added Docker startup script to simplify Docker startup; Executor can be directly connected to Ollama to use the models and applications supported by Ollama; You can use update.bat to quickly update to the latest released version without re-downloading the .exe installer [!Note] kuwa-v0.3.0 Download information: https://github.com/kuwaai/genai-os/releases/tag/v0.3.0 kuwa-taide-v0.3.0 Single executable download link: https://github.com/kuwaai/genai-os/releases/download/v0.3.0/kuwa-taide-v0.3.0.exe In addition, we also provide some Kuwa tutorial documents: DB QA/RAG Setup Tutorial: https://kuwaai.tw/blog/dbqa-setup Bot Setup Tutorial: https://kuwaai.tw/blog/bot-system-guide Docker Startup Tutorial: https://github.com/kuwaai/genai-os/blob/main/docker/README_TW.md SearchQA Setup Tutorial: https://kuwaai.tw/blog/search-qa-setup Gemini API Key Application Tutorial: https://kuwaai.tw/blog/apply-gemini We welcome everyone to give feedback after experiencing it, and if you encounter any difficulties, please feel free to contact us through the community or other channels. Kuwa official website: https://kuwaai.tw/","title":"2024 05 19 release v0.3.0"},{"location":"release/2024-06-24-relese-v0.3.1/","text":"Hi everyone, Kuwa v0.3.1 is out, and this update mainly focuses on multimodal input and output, which now supports both speech and images. Combined with the previously launched Bot system and group chat functions, this allows for practical functions such as meeting summaries, speech summaries, simple image generation, and image editing: 1. Supports the Whisper speech-to-text model , which can output transcripts from uploaded audio files, and features multi-speaker recognition and timestamps. 2. Supports the Stable Diffusion image generation model , which can generate images from text input or modify uploaded images based on user instructions. 3. Huggingface executor supports integration with vision-language models such as Phi-3-Vision and LLaVA. 4. RAG supports direct parameter adjustment through the Web UI and Modelfile , simplifying the calibration process. 5. RAG supports displaying original documents and cited passages , making it easier to review search results and identify hallucinations. 6. Supports importing pre-built RAG vector databases , facilitating knowledge sharing across different systems. 7. Simplified selection of various open models during installation. 8. Multi-chat Web UI supports direct export of chat records in PDF, Doc/ODT formats . 9. Multi-chat Web UI supports Modelfile syntax highlighting, making it easy to edit Modelfiles. 10. Kernel API supports passing website language, allowing the Executor to customize based on user language. 11. The Executor removes the default System prompt to avoid compromising model performance. [!Note] kuwa-v0.3.1 Download information: https://github.com/kuwaai/genai-os/releases/tag/v0.3.1 kuwa-v0.3.1 Single executable download link: https://github.com/kuwaai/genai-os/releases/download/v0.3.1/Kuwa-GenAI-OS-v0.3.1.exe Here are the detailed user guide documents: 1. Whisper Speech-to-Text Model User Guide (including speaker recognition): https://kuwaai.tw/blog/whisper-tutorial 2. Stable Diffusion Image Generation Model User Guide: https://kuwaai.tw/blog/diffusion-tutorial 3. Vision and Language Model Integration Tutorial: https://kuwaai.tw/blog/vlm-tutorial We welcome your feedback after trying out the new version, and please feel free to contact us through the community or other channels if you encounter any difficulties. Official Kuwa website: https://kuwaai.tw/ Introduction to Kuwa GenAI OS Kuwa GenAI OS is a free, open, secure, and privacy-focused open-source system that provides a user-friendly interface for generative AI and a new-generation generative AI orchestrator system that supports rapid development of LLM applications. Kuwa provides an end-to-end solution for multilingual and multi-model development and deployment, empowering individuals and industries to use generative AI on local laptops, servers or the cloud, develop applications, or open stores and provide services externally. Here is a brief description of version v0.3.1: Usage Environment Supports multiple operating systems including Windows, Linux, and MacOS, and provides easy installation and software update tools, such as a single installation executable for Windows, an automatic installation script for Linux, a Docker startup script, and a pre-installed VM virtual machine. Supports a variety of hardware environments , from Raspberry Pi, laptops, personal computers, and on-premises servers to virtual hosts, public and private clouds, with or without GPU accelerators. User Interface The integrated interface can select any model, knowledge base, or GenAI application, and combine them to create single or group chat rooms. The chat room can be self-directed , citing dialogue, specifying group chat or direct private chat, switching between continuous Q&A mode or single-question Q&A mode Controllable crossings at any time, import prompt scripts or upload files, you can also export complete chat room conversation scripts, directly output files in formats such as PDF, Doc/ODT, plain text, or share web pages Supports text, image generation, speech, and visual recognition multimodal language models , and can highlight syntax such as programming and Markdown, or quickly use system gadgets. Development Interface Users can skip coding by connecting existing models, knowledge bases, or Bot applications, adjusting system prompts and parameters, presetting scenarios, or creating prompt templates to create personalized or more powerful GenAI applications. Users can create their own knowledge base by simple drag and drop, or import existing vector databases, and can use multiple knowledge bases for GenAI applications at the same time. Users can create and maintain their own shared app Store , and users can also share bot apps The Kuwa extension model and RAG advanced functions can be adjusted and enabled through the Ollama modelfile. Deployment Interface Supports multiple languages, can customize the interface and messages, and directly provide services for external deployment . Existing accounts can be connected or registered with an invitation code. When the password is forgotten, it can be reset with Email. System settings can modify system announcements, terms of service, warnings, etc., or perform group permission management, user management, model management, etc. The dashboard supports feedback management, system log management, security and privacy management, message query, etc. Development Environment Integrates a variety of open-source generative AI tools , including Faiss, HuggingFace, Langchain, llama.cpp, Ollama, vLLM, and various Embedding and Transformer-related packages. Developers can download, connect, and develop various multimodal LLMs and applications. RAG Toolchain includes multiple search-augmented generation application tools such as DBQA, DocumentQA, WebQA, and SearchQA, which can be connected with search engines and automatic crawlers, or integrated with existing corporate databases or systems, facilitating the development of advanced customized applications. Open source allows developers to create their own custom systems based on their own needs.","title":"2024 06 24 relese v0.3.1"},{"location":"release/2024-06-24-relese-v0.3.1/#introduction-to-kuwa-genai-os","text":"Kuwa GenAI OS is a free, open, secure, and privacy-focused open-source system that provides a user-friendly interface for generative AI and a new-generation generative AI orchestrator system that supports rapid development of LLM applications. Kuwa provides an end-to-end solution for multilingual and multi-model development and deployment, empowering individuals and industries to use generative AI on local laptops, servers or the cloud, develop applications, or open stores and provide services externally. Here is a brief description of version v0.3.1:","title":"Introduction to Kuwa GenAI OS"},{"location":"release/2024-06-24-relese-v0.3.1/#usage-environment","text":"Supports multiple operating systems including Windows, Linux, and MacOS, and provides easy installation and software update tools, such as a single installation executable for Windows, an automatic installation script for Linux, a Docker startup script, and a pre-installed VM virtual machine. Supports a variety of hardware environments , from Raspberry Pi, laptops, personal computers, and on-premises servers to virtual hosts, public and private clouds, with or without GPU accelerators.","title":"Usage Environment"},{"location":"release/2024-06-24-relese-v0.3.1/#user-interface","text":"The integrated interface can select any model, knowledge base, or GenAI application, and combine them to create single or group chat rooms. The chat room can be self-directed , citing dialogue, specifying group chat or direct private chat, switching between continuous Q&A mode or single-question Q&A mode Controllable crossings at any time, import prompt scripts or upload files, you can also export complete chat room conversation scripts, directly output files in formats such as PDF, Doc/ODT, plain text, or share web pages Supports text, image generation, speech, and visual recognition multimodal language models , and can highlight syntax such as programming and Markdown, or quickly use system gadgets.","title":"User Interface"},{"location":"release/2024-06-24-relese-v0.3.1/#development-interface","text":"Users can skip coding by connecting existing models, knowledge bases, or Bot applications, adjusting system prompts and parameters, presetting scenarios, or creating prompt templates to create personalized or more powerful GenAI applications. Users can create their own knowledge base by simple drag and drop, or import existing vector databases, and can use multiple knowledge bases for GenAI applications at the same time. Users can create and maintain their own shared app Store , and users can also share bot apps The Kuwa extension model and RAG advanced functions can be adjusted and enabled through the Ollama modelfile.","title":"Development Interface"},{"location":"release/2024-06-24-relese-v0.3.1/#deployment-interface","text":"Supports multiple languages, can customize the interface and messages, and directly provide services for external deployment . Existing accounts can be connected or registered with an invitation code. When the password is forgotten, it can be reset with Email. System settings can modify system announcements, terms of service, warnings, etc., or perform group permission management, user management, model management, etc. The dashboard supports feedback management, system log management, security and privacy management, message query, etc.","title":"Deployment Interface"},{"location":"release/2024-06-24-relese-v0.3.1/#development-environment","text":"Integrates a variety of open-source generative AI tools , including Faiss, HuggingFace, Langchain, llama.cpp, Ollama, vLLM, and various Embedding and Transformer-related packages. Developers can download, connect, and develop various multimodal LLMs and applications. RAG Toolchain includes multiple search-augmented generation application tools such as DBQA, DocumentQA, WebQA, and SearchQA, which can be connected with search engines and automatic crawlers, or integrated with existing corporate databases or systems, facilitating the development of advanced customized applications. Open source allows developers to create their own custom systems based on their own needs.","title":"Development Environment"},{"location":"release/2024-07-05-release-v0.3.2/","text":"Feature Updates Customized Bot Permissions: Configure the Bot's readable and executable permissions at system, community, group, and individual levels Customized Upload File Policy: Admin can set maximum upload file size and allowed file types Tool Samples: Added samples for Copycat, token counter, etc. Pre-defined Model Profiles: Provided profiles for LLaVA and other fine-tuned models UX Optimization: Beautified icons and chat lists Updated Default Models: ChatGPT Executor is connected to GPT-4o by default, Gemini Executor is connected to Gemini 1.5 pro by default Bug Fixes File name with whitespace parsing issue when uploading Language is not saved after logout Dependency issue of Llamacpp Executor Color and line breaks not supported in Windows version logs The first message in the group chat is always sent even using multi-chat single-turn Q&A Windows version DocQA default parameters may exceed the context window New Tutorials Customizing RAG Parameters Tutorial: https://kuwaai.tw/blog/rag-param-tutorial Customizing Tool Tutorial: https://kuwaai.tw/blog/rag-param-tutorial [!NOTE] kuwa-v0.3.1 Download information: https://github.com/kuwaai/genai-os/releases/tag/v0.3.2 kuwa-v0.3.1 Single executable download link: https://github.com/kuwaai/genai-os/releases/download/v0.3.2/Kuwa-GenAI-OS-v0.3.2.exe [!WARNING] Known issue: In v0.3.2 Windows version, the tee.bat file in the GenAI OS\\windows\\src directory is mistakenly detected as a virus by Microsoft Defender and quarantined, causing Kuwa to crash when opened. Solution: Refer to this document to restore the file and reopen Kuwa after it has been restored. Welcome to provide feedback after experience. Please contact us via the community or other channels if you encounter any issues. Kuwa Official Website: https://kuwaai.tw/ Kuwa Introduction: https://kuwaai.tw/blog/kuwa-os-intro","title":"2024 07 05 release v0.3.2"},{"location":"release/2024-07-05-release-v0.3.2/#feature-updates","text":"Customized Bot Permissions: Configure the Bot's readable and executable permissions at system, community, group, and individual levels Customized Upload File Policy: Admin can set maximum upload file size and allowed file types Tool Samples: Added samples for Copycat, token counter, etc. Pre-defined Model Profiles: Provided profiles for LLaVA and other fine-tuned models UX Optimization: Beautified icons and chat lists Updated Default Models: ChatGPT Executor is connected to GPT-4o by default, Gemini Executor is connected to Gemini 1.5 pro by default","title":"Feature Updates"},{"location":"release/2024-07-05-release-v0.3.2/#bug-fixes","text":"File name with whitespace parsing issue when uploading Language is not saved after logout Dependency issue of Llamacpp Executor Color and line breaks not supported in Windows version logs The first message in the group chat is always sent even using multi-chat single-turn Q&A Windows version DocQA default parameters may exceed the context window","title":"Bug Fixes"},{"location":"release/2024-07-05-release-v0.3.2/#new-tutorials","text":"Customizing RAG Parameters Tutorial: https://kuwaai.tw/blog/rag-param-tutorial Customizing Tool Tutorial: https://kuwaai.tw/blog/rag-param-tutorial [!NOTE] kuwa-v0.3.1 Download information: https://github.com/kuwaai/genai-os/releases/tag/v0.3.2 kuwa-v0.3.1 Single executable download link: https://github.com/kuwaai/genai-os/releases/download/v0.3.2/Kuwa-GenAI-OS-v0.3.2.exe [!WARNING] Known issue: In v0.3.2 Windows version, the tee.bat file in the GenAI OS\\windows\\src directory is mistakenly detected as a virus by Microsoft Defender and quarantined, causing Kuwa to crash when opened. Solution: Refer to this document to restore the file and reopen Kuwa after it has been restored. Welcome to provide feedback after experience. Please contact us via the community or other channels if you encounter any issues. Kuwa Official Website: https://kuwaai.tw/ Kuwa Introduction: https://kuwaai.tw/blog/kuwa-os-intro","title":"New Tutorials"},{"location":"release/2024-08-12-release-v0.3.3/","text":"Feature Updates Added Pipe executor, which can execute programs (tools) within a specified directory , such as directly executing Python programs output by models via Python interpreter Provided Calculator, Iconv and Python example tools that can be called via Pipe executor Added Uploader executor to allow users to upload files to a specified directory, including tools, RAG knowledge bases, or website components Supported Bot export and import , allowing export of Bot name, description, icon, and Modelfile as a single Bot file, similar to an application configuration file; installation can automatically import default Bot files Allows users to choose the sorting method for Bots in chat rooms Supported Bot icon replacement Added Kuwa API server compatible with OpenAI API Provided default examples for connecting to cloud multimodal APIs : gpt-4o-mini-vision, DALL-E, Gemini pro 1.5 vision Supported setting the upper limit of uploaded files via Web interface Supported installation and execution in environments with Web proxy within enterprises Supported acceleration of model inference using Intel GPU Added automatic installation and update scripts for Docker version , thanks to @wcwutw RAG Toolchain default Embedding model replaced with Microsoft's intfloat/multilingual-e5-small model, licensed under MIT RAG (DocQA, WebQA, SearchQA, DB QA) added display_hide_ref_content, retriever_ttl_sec parameters Increased support for downloaded tools' default models, including Meta Llama 3.1 8B with Function calling and lightweight Google Gemma 2 2B Bug Fixes #21 : Docker version does not generate https:// links after a reverse proxy, thanks to @Phate334 #23 : Two-minute timeout issue, thanks to @x85432 #24 : Modelfile parsing issue #25 : Importing Prompts does not apply Modelfile windows\\src\\tee.bat is misjudged as a virus RAG reference data does not display original file names Updated Windows version dependency download link [!Note] - Kuwa-v0.3.3 download information: https://github.com/kuwaai/genai-os/releases/tag/v0.3.3 - Kuwa-v0.3.3 single executable download link: https://github.com/kuwaai/genai-os/releases/download/v0.3.3/Kuwa-GenAI-OS-v0.3.3.exe [!Warning] 1. Windows version may encounter 502 Bad Gateway error when upgrading from old version, please remove C:\\kuwa\\GenAI OS\\windows\\packages\\nginx-1.24.0\\conf\\nginx.conf and re-run build.bat once 2. Pipe executor can execute any program and currently lacks Chroot, please understand related risks before providing external services We welcome your feedback after experiencing it. If you encounter any difficulties, please feel free to contact us through community or other channels. Kuwa official website: https://kuwaai.tw/ Kuwa introduction: https://kuwaai.tw/blog/kuwa-os-intro","title":"2024 08 12 release v0.3.3"},{"location":"release/2024-08-12-release-v0.3.3/#feature-updates","text":"Added Pipe executor, which can execute programs (tools) within a specified directory , such as directly executing Python programs output by models via Python interpreter Provided Calculator, Iconv and Python example tools that can be called via Pipe executor Added Uploader executor to allow users to upload files to a specified directory, including tools, RAG knowledge bases, or website components Supported Bot export and import , allowing export of Bot name, description, icon, and Modelfile as a single Bot file, similar to an application configuration file; installation can automatically import default Bot files Allows users to choose the sorting method for Bots in chat rooms Supported Bot icon replacement Added Kuwa API server compatible with OpenAI API Provided default examples for connecting to cloud multimodal APIs : gpt-4o-mini-vision, DALL-E, Gemini pro 1.5 vision Supported setting the upper limit of uploaded files via Web interface Supported installation and execution in environments with Web proxy within enterprises Supported acceleration of model inference using Intel GPU Added automatic installation and update scripts for Docker version , thanks to @wcwutw RAG Toolchain default Embedding model replaced with Microsoft's intfloat/multilingual-e5-small model, licensed under MIT RAG (DocQA, WebQA, SearchQA, DB QA) added display_hide_ref_content, retriever_ttl_sec parameters Increased support for downloaded tools' default models, including Meta Llama 3.1 8B with Function calling and lightweight Google Gemma 2 2B","title":"Feature Updates"},{"location":"release/2024-08-12-release-v0.3.3/#bug-fixes","text":"#21 : Docker version does not generate https:// links after a reverse proxy, thanks to @Phate334 #23 : Two-minute timeout issue, thanks to @x85432 #24 : Modelfile parsing issue #25 : Importing Prompts does not apply Modelfile windows\\src\\tee.bat is misjudged as a virus RAG reference data does not display original file names Updated Windows version dependency download link [!Note] - Kuwa-v0.3.3 download information: https://github.com/kuwaai/genai-os/releases/tag/v0.3.3 - Kuwa-v0.3.3 single executable download link: https://github.com/kuwaai/genai-os/releases/download/v0.3.3/Kuwa-GenAI-OS-v0.3.3.exe [!Warning] 1. Windows version may encounter 502 Bad Gateway error when upgrading from old version, please remove C:\\kuwa\\GenAI OS\\windows\\packages\\nginx-1.24.0\\conf\\nginx.conf and re-run build.bat once 2. Pipe executor can execute any program and currently lacks Chroot, please understand related risks before providing external services We welcome your feedback after experiencing it. If you encounter any difficulties, please feel free to contact us through community or other channels. Kuwa official website: https://kuwaai.tw/ Kuwa introduction: https://kuwaai.tw/blog/kuwa-os-intro","title":"Bug Fixes"},{"location":"tutorial/2024-04-20-llama3-linux/","text":"Getting the Model Method 1: Applying for Access on HuggingFace Log in to HuggingFace and apply for access to the meta-llama/Meta-Llama-3-8B-Instruct model (approximately 1 hour review time) If you see the \"You have been granted access to this model\" message, you have obtained the model access, and you can download the model If you need to use a model that requires login, you need to set up a HuggingFace token. If you are using a model that does not require login, you can skip this step Go to https://huggingface.co/settings/tokens?new_token=true Enter your desired name Then, keep this token safe (do not share it with anyone) Method 2: Direct Download from HuggingFace If you don't want to log in to HuggingFace, you can find a third-party re-uploaded model (named Meta-Llama-3-8B-Instruct, no GGUF): HuggingFace search: https://huggingface.co/models?search=Meta-Llama-3-8B-Instruct For example, NousResearch/Meta-Llama-3-8B-Instruct, remember the name 2. Kuwa Settings Method 1: Starting Executor using Command You can start the Llama3 8B Instruct Executor (with code llama3-8b-instruct ) using the following command, replacing <YOUR_HF_TOKEN> with the HuggingFace token obtained in the previous step. If you downloaded it from a third-party, leave it blank. The --model_path parameter is followed by the name of the model on the Huggingface hub. You can obtain the model using method 1: meta-llama/Meta-Llama-3-8B-Instruct or method 2: NousResearch/Meta-Llama-3-8B-Instruct . sh export HUGGING_FACE_HUB_TOKEN=<YOUR_HF_TOKEN> kuwa-executor huggingface --access_code llama3-8b-instruct --log debug --model_path meta-llama/Meta-Llama-3-8B-Instruct --stop \" --no_system_prompt sh export HUGGING_FACE_HUB_TOKEN= kuwa-executor huggingface --access_code llama3-8b-instruct --log debug --model_path NousResearch/Meta-Llama-3-8B-Instruct --stop \" --no_system_prompt After adding the Llama3 8B Instruct model settings in the web frontend, you can use it. Method 2: Starting Executor using Docker Create a llama3.yaml file in the genai-os/docker/ directory and fill in the following content. If you use method 1, you need to modify the command parameter in the compose file to meta-llama/Meta-Llama-3-8B-Instruct . services: llama3-executor: build: context: ../ dockerfile: docker/executor/Dockerfile image: kuwa-executor environment: EXECUTOR_TYPE: huggingface EXECUTOR_ACCESS_CODE: llama3-8b-instruct EXECUTOR_NAME: Meta Llama3 8B Instruct # HUGGING_FACE_HUB_TOKEN: ${HUGGING_FACE_HUB_TOKEN} depends_on: - kernel - multi-chat command: [\"--model_path\", \"NousResearch/Meta-Llama-3-8B-Instruct\", \"--no_system_prompt\", \"--stop\", \"<|eot_id|>\"] restart: unless-stopped volumes: [\"~/.cache/huggingface:/root/.cache/huggingface\"] deploy: resources: reservations: devices: - driver: nvidia device_ids: ['0'] capabilities: [gpu] networks: [\"backend\"] Use the following command to start a new container ( <...> is the existing system's compose file combination, and the existing system does not need to be stopped). sudo docker compose -f compose.yaml <...> -f llama3.yaml up --build If the Executor runs successfully, you will see the following image. 3. Kuwa Usage Wait for the model to download and then log in to Kuwa. You can start chatting with Llama3. Llama3 is set to prefer English, and you can use the \"Translate this model's response\" function to translate the model's response into Chinese. You can use the group chat function to compare the responses of Llama3, Llama2, and TAIDE-LX-7B-Chat.","title":"2024 04 20 llama3 linux"},{"location":"tutorial/2024-04-20-llama3-linux/#getting-the-model","text":"","title":"Getting the Model"},{"location":"tutorial/2024-04-20-llama3-linux/#method-1-applying-for-access-on-huggingface","text":"Log in to HuggingFace and apply for access to the meta-llama/Meta-Llama-3-8B-Instruct model (approximately 1 hour review time) If you see the \"You have been granted access to this model\" message, you have obtained the model access, and you can download the model If you need to use a model that requires login, you need to set up a HuggingFace token. If you are using a model that does not require login, you can skip this step Go to https://huggingface.co/settings/tokens?new_token=true Enter your desired name Then, keep this token safe (do not share it with anyone)","title":"Method 1: Applying for Access on HuggingFace"},{"location":"tutorial/2024-04-20-llama3-linux/#method-2-direct-download-from-huggingface","text":"If you don't want to log in to HuggingFace, you can find a third-party re-uploaded model (named Meta-Llama-3-8B-Instruct, no GGUF): HuggingFace search: https://huggingface.co/models?search=Meta-Llama-3-8B-Instruct For example, NousResearch/Meta-Llama-3-8B-Instruct, remember the name","title":"Method 2: Direct Download from HuggingFace"},{"location":"tutorial/2024-04-20-llama3-linux/#2-kuwa-settings","text":"","title":"2. Kuwa Settings"},{"location":"tutorial/2024-04-20-llama3-linux/#method-1-starting-executor-using-command","text":"You can start the Llama3 8B Instruct Executor (with code llama3-8b-instruct ) using the following command, replacing <YOUR_HF_TOKEN> with the HuggingFace token obtained in the previous step. If you downloaded it from a third-party, leave it blank. The --model_path parameter is followed by the name of the model on the Huggingface hub. You can obtain the model using method 1: meta-llama/Meta-Llama-3-8B-Instruct or method 2: NousResearch/Meta-Llama-3-8B-Instruct . sh export HUGGING_FACE_HUB_TOKEN=<YOUR_HF_TOKEN> kuwa-executor huggingface --access_code llama3-8b-instruct --log debug --model_path meta-llama/Meta-Llama-3-8B-Instruct --stop \" --no_system_prompt sh export HUGGING_FACE_HUB_TOKEN= kuwa-executor huggingface --access_code llama3-8b-instruct --log debug --model_path NousResearch/Meta-Llama-3-8B-Instruct --stop \" --no_system_prompt After adding the Llama3 8B Instruct model settings in the web frontend, you can use it.","title":"Method 1: Starting Executor using Command"},{"location":"tutorial/2024-04-20-llama3-linux/#method-2-starting-executor-using-docker","text":"Create a llama3.yaml file in the genai-os/docker/ directory and fill in the following content. If you use method 1, you need to modify the command parameter in the compose file to meta-llama/Meta-Llama-3-8B-Instruct . services: llama3-executor: build: context: ../ dockerfile: docker/executor/Dockerfile image: kuwa-executor environment: EXECUTOR_TYPE: huggingface EXECUTOR_ACCESS_CODE: llama3-8b-instruct EXECUTOR_NAME: Meta Llama3 8B Instruct # HUGGING_FACE_HUB_TOKEN: ${HUGGING_FACE_HUB_TOKEN} depends_on: - kernel - multi-chat command: [\"--model_path\", \"NousResearch/Meta-Llama-3-8B-Instruct\", \"--no_system_prompt\", \"--stop\", \"<|eot_id|>\"] restart: unless-stopped volumes: [\"~/.cache/huggingface:/root/.cache/huggingface\"] deploy: resources: reservations: devices: - driver: nvidia device_ids: ['0'] capabilities: [gpu] networks: [\"backend\"] Use the following command to start a new container ( <...> is the existing system's compose file combination, and the existing system does not need to be stopped). sudo docker compose -f compose.yaml <...> -f llama3.yaml up --build If the Executor runs successfully, you will see the following image.","title":"Method 2: Starting Executor using Docker"},{"location":"tutorial/2024-04-20-llama3-linux/#3-kuwa-usage","text":"Wait for the model to download and then log in to Kuwa. You can start chatting with Llama3. Llama3 is set to prefer English, and you can use the \"Translate this model's response\" function to translate the model's response into Chinese. You can use the group chat function to compare the responses of Llama3, Llama2, and TAIDE-LX-7B-Chat.","title":"3. Kuwa Usage"},{"location":"tutorial/2024-04-20-llama3-windows/","text":"Getting the Model Method 1: Applying for Access on HuggingFace Log in to HuggingFace and go to https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct to apply for access to the meta-llama/Meta-Llama-3-8B-Instruct model (approximately 1 hour for review) If you see the \"You have been granted access to this model\" message, it means you have obtained the model access, and you can download the model If you need to use a model that requires login, you need to set up the HuggingFace Token. If you are using a model that does not require login, you can skip this step Go to https://huggingface.co/settings/tokens?new_token=true Enter your desired name Then, keep this token safe (do not share it with anyone) Next, go to the project directory's kuwa\\GenAI OS\\windows folder and execute tool.bat Enter the HF Token you just generated, and you can use the mouse right-click to paste it, but this input is invisible, so enter it and press Enter Enter n for the Git certificate part After that, you will see \"Login successful\" to indicate that the setting is successful. Method 2: Direct Download from HuggingFace without Login If you don't want to log in to HuggingFace, you can find a third-party re-uploaded model (named Meta-Llama-3-8B-Instruct, without GGUF): Search on HuggingFace: https://huggingface.co/models?search=Meta-Llama-3-8B-Instruct For example, NousResearch/Meta-Llama-3-8B-Instruct , remember the name Setting up Kuwa Go to the kuwa\\GenAI OS\\windows\\executors folder, which should have a huggingface subfolder by default, enter it, and open init.bat You need to enter the model path, which can be the location on HuggingFace, such as: Method 1: meta-llama/Meta-Llama-3-8B-Instruct Method 2: NousResearch/Meta-Llama-3-8B-Instruct The image part will automatically find the image in the folder Arguments to use: \"--no_system_prompt\" \"--stop\" \"<|eot_id|>\" ; if you want to customize parameters, please refer to the README file in the executor folder This will automatically create a run.bat file in the kuwa\\GenAI OS\\windows\\executors\\huggingface folder Go back to the project directory's kuwa\\GenAI OS\\windows folder and execute start.bat to automatically download and start the model. Note: The downloaded models will be stored in the .cache\\huggingface\\hub folder in the user directory, and if the space is insufficient, please clean up the model cache. Using Kuwa Wait for the model to download and then log in to Kuwa to start chatting with Llama3 Llama3 is set to prefer English, and you can use the \"Translate this model's response\" function to translate the model's response into Chinese You can use the group chat function to compare the responses of Llama3, Llama2, and TAIDE-LX-7B-Chat","title":"2024 04 20 llama3 windows"},{"location":"tutorial/2024-04-20-llama3-windows/#getting-the-model","text":"","title":"Getting the Model"},{"location":"tutorial/2024-04-20-llama3-windows/#method-1-applying-for-access-on-huggingface","text":"Log in to HuggingFace and go to https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct to apply for access to the meta-llama/Meta-Llama-3-8B-Instruct model (approximately 1 hour for review) If you see the \"You have been granted access to this model\" message, it means you have obtained the model access, and you can download the model If you need to use a model that requires login, you need to set up the HuggingFace Token. If you are using a model that does not require login, you can skip this step Go to https://huggingface.co/settings/tokens?new_token=true Enter your desired name Then, keep this token safe (do not share it with anyone) Next, go to the project directory's kuwa\\GenAI OS\\windows folder and execute tool.bat Enter the HF Token you just generated, and you can use the mouse right-click to paste it, but this input is invisible, so enter it and press Enter Enter n for the Git certificate part After that, you will see \"Login successful\" to indicate that the setting is successful.","title":"Method 1: Applying for Access on HuggingFace"},{"location":"tutorial/2024-04-20-llama3-windows/#method-2-direct-download-from-huggingface-without-login","text":"If you don't want to log in to HuggingFace, you can find a third-party re-uploaded model (named Meta-Llama-3-8B-Instruct, without GGUF): Search on HuggingFace: https://huggingface.co/models?search=Meta-Llama-3-8B-Instruct For example, NousResearch/Meta-Llama-3-8B-Instruct , remember the name","title":"Method 2: Direct Download from HuggingFace without Login"},{"location":"tutorial/2024-04-20-llama3-windows/#setting-up-kuwa","text":"Go to the kuwa\\GenAI OS\\windows\\executors folder, which should have a huggingface subfolder by default, enter it, and open init.bat You need to enter the model path, which can be the location on HuggingFace, such as: Method 1: meta-llama/Meta-Llama-3-8B-Instruct Method 2: NousResearch/Meta-Llama-3-8B-Instruct The image part will automatically find the image in the folder Arguments to use: \"--no_system_prompt\" \"--stop\" \"<|eot_id|>\" ; if you want to customize parameters, please refer to the README file in the executor folder This will automatically create a run.bat file in the kuwa\\GenAI OS\\windows\\executors\\huggingface folder Go back to the project directory's kuwa\\GenAI OS\\windows folder and execute start.bat to automatically download and start the model. Note: The downloaded models will be stored in the .cache\\huggingface\\hub folder in the user directory, and if the space is insufficient, please clean up the model cache.","title":"Setting up Kuwa"},{"location":"tutorial/2024-04-20-llama3-windows/#using-kuwa","text":"Wait for the model to download and then log in to Kuwa to start chatting with Llama3 Llama3 is set to prefer English, and you can use the \"Translate this model's response\" function to translate the model's response into Chinese You can use the group chat function to compare the responses of Llama3, Llama2, and TAIDE-LX-7B-Chat","title":"Using Kuwa"},{"location":"tutorial/2024-05-18-bot-system-guide/","text":"In the latest 0.3.0 release, we have mainly added the Bot feature. Now you can create different Bots on the website, and this article will guide you step-by-step on how to set up your own Bot! First, you will see a Store section:\\ \\ With a button outlined in green, clicking it will open the Bot creation menu.\\ \\ Here, you have a simple interface to set common Bot parameters, such as system prompts, user pre-prompts, and user post-prompts. If you want to set more detailed information, you can also open the model configuration file:\\ \\ Although this part does not have an auxiliary interface, you can more freely set all parameters. Please refer to the Ollama Modelfile for the format. Note that in the current 0.3.0 version, only some configuration parameters are supported. The following lists the relevant parameters and some example usages. SYSTEM \\ \\ The system prompt should serve as the main method to influence the model's output, preloading some knowledge or changing the response style. SYSTEM You are a helpful assistant. SYSTEM Please respond briefly. SYSTEM Your name is Bob, and you love learning other languages. TEMPLATE \\ \\ Specify the dialogue template to apply during inference. The template used by each model may vary, so it is recommended to refer to the relevant template for the model. TEMPLATE \"\"\" {% for message in messages %}\\ {% if message['role'] == 'system' %}\\ {{ '\\ ' + message['content'] }}\\ {% endif %}\\ {% if message['role'] == 'user' %}\\ {{ 'USER: ' + message['content'] }}\\ {% endif %}\\ {% if message['role'] == 'assistant' %}\\ {{ 'ASSISTANT: ' + message['content'] }}\\ {% endif %}\\ {% endfor %}\\ {{ 'ASSISTANT: ' }}\"\"\" MESSAGE \\ \\ \\ Preload some dialogue records. The User and Assistant parts must be paired. MESSAGE SYSTEM You are a helpful assistant. MESSAGE SYSTEM Please respond briefly. MESSAGE USER Hello. MESSAGE ASSISTANT \"\"\"Hello! How can I assist you?\"\"\" In addition to the parameters supported by the original modelfile, we have also extended two additional parameters: BEFORE-PROMPT \\ \\ In the last message, this prompt will be placed before the user's message. BEFORE-PROMPT Please translate the following into Japanese: \u300c BEFORE-PROMPT \u300c AFTER-PROMPT \\ \\ In the last message, this prompt will be placed after the user's message. AFTER-PROMPT \u300d AFTER-PROMPT \u300d, please rephrase the above content. Here are some example modelfile configurations: Automatically add \"Please translate into Japanese\": TEMPLATE \"\"\"{% for message in messages %} {% if message['role'] == 'system' %} {{ '<s>' + message['content'] }} {% endif %} {% if message['role'] == 'user' %} {{' USER: Please translate into Japanese: ' + message['content']}} {% endif %} {% if message['role'] == 'assistant' %} {{' ASSISTANT: ' + message['content']}} {% endif %} {% endfor %} {{' ASSISTANT: '}}\"\"\" Pretend to be a cat: SYSTEM You are a cat. Regardless of what I ask, you should only respond with \"meow\" or \"mew\" and not speak any human language or act like anything else. Meow Translator: BEFORE-PROMPT Please replace the following message entirely with \"meow\": \u300c AFTER-PROMPT \u300d Bilingual Teacher: SYSTEM You are a helpful English teacher who corrects grammar and provides answers in both Chinese and English. TAIDE Chinese Proofreader: SYSTEM You are a professional Chinese teacher with expertise in proofreading and editing in fluent Traditional Chinese from Taiwan. BEFORE-PROMPT Please directly refine the following text in Chinese without explanation: TAIDE Chinese to English Translator: SYSTEM You are a professional English teacher helping translate content into English from a Taiwanese perspective. BEFORE-PROMPT Translate into English without explanation: \u300c AFTER-PROMPT \u300d Chinese Chatting: (For use with ChatGPT, Gemini) AFTER-PROMPT Please answer in Traditional Chinese from a Taiwanese perspective. Taiwan Search QA: (For use with Search QA) SYSTEM site:tw Answer in Traditional Chinese. Define some preset knowledge: SYSTEM Your name is Jeff. MESSAGE user What is your name? MESSAGE assistant My name is Jeff. Hello! MESSAGE user When I say ABCD, please respond with \"EFGH!!!!\" MESSAGE assistant Okay, I will shout \"EFGH!!!!\" when you mention ABCD. MESSAGE user ABCD? MESSAGE assistant EFGH!!!! MESSAGE user ABCDEFG? MESSAGE assistant EFGH!!!! MESSAGE user What comes after ABC? MESSAGE assistant DEFGHIJKLMNOPQRSTUVWXYZ MESSAGE user It rained heavily today. MESSAGE assistant Noted, it was a torrential rain today. Please note that not all models support these parameters. For example, the current Gemini API does not support templates. The system prompt part is supported as a before-prompt. Additionally, ChatGPT does not support template settings. The effectiveness of these settings depends on the model's training. If the training for system prompts is insufficient, it may be challenging to influence the model's behavior using the system prompt alone. You can try to influence the model output using MESSAGE or Before/After prompt instead.","title":"2024 05 18 bot system guide"},{"location":"tutorial/2024-05-19-apply-gemini/","text":"Even if the model is not downloaded on the client side, you can use Kuwa to connect to the cloud model for chat and RAG applications. This article will guide you through the process of applying for Google's free Gemini API key and configuring it in Kuwa. Applying for a Gemini API Key Visit Google AI Studio - Get API key and click \"Get API key\" Click \"Create API key\" to create a new key You can select an existing Google Cloud Project or create a new one Note down the API key. There are two ways to set it up later, personal or global API key, choose one Method 1: Set up Personal API Key Personal API key is only valid for the current Kuwa account and no one else can use your API key. Log in to your Kuwa account, and select \"Settings\" from the account menu in the top right corner. In the \"API Management\" section, fill in the Google API key you just applied for, and click \"Update\" to start using Gemini. Method 2: Set Up Global API Key Global API key is suitable for organizations that have purchased Google Gemini services. It allows all website users to use Gemini directly without setting up API keys. Windows Tutorial Run C:\\kuwa\\GenAI OS\\windows\\executors\\geminipro\\init.bat , and fill in the Google API key you just applied for. Restart Kuwa, or go back to the command line of Kuwa GenAI OS and input reload to reload all executors and start using Gemini. Docker Tutorial Refer to docker/compose/gemini.yaml to create a Gemini executor. Replace <YOUR_GLOBAL_API_KEY_HERE> with the Google API key you applied in the previous section. yaml services: gemini-executor: build: context: ../../ dockerfile: docker/executor/Dockerfile image: kuwa-executor environment: EXECUTOR_TYPE: geminipro EXECUTOR_ACCESS_CODE: geminipro EXECUTOR_NAME: Gemini depends_on: - kernel - multi-chat command: [\"--api_key\", \"<YOUR_GLOBAL_API_KEY_HERE>\"] restart: unless-stopped networks: [\"backend\"] 2. Add gemini to the confs array in docker/run.sh , and re-run docker/run.sh to start the Gemini executor.","title":"2024 05 19 apply gemini"},{"location":"tutorial/2024-05-19-apply-gemini/#applying-for-a-gemini-api-key","text":"Visit Google AI Studio - Get API key and click \"Get API key\" Click \"Create API key\" to create a new key You can select an existing Google Cloud Project or create a new one Note down the API key. There are two ways to set it up later, personal or global API key, choose one","title":"Applying for a Gemini API Key"},{"location":"tutorial/2024-05-19-apply-gemini/#method-1-set-up-personal-api-key","text":"Personal API key is only valid for the current Kuwa account and no one else can use your API key. Log in to your Kuwa account, and select \"Settings\" from the account menu in the top right corner. In the \"API Management\" section, fill in the Google API key you just applied for, and click \"Update\" to start using Gemini.","title":"Method 1: Set up Personal API Key"},{"location":"tutorial/2024-05-19-apply-gemini/#method-2-set-up-global-api-key","text":"Global API key is suitable for organizations that have purchased Google Gemini services. It allows all website users to use Gemini directly without setting up API keys.","title":"Method 2: Set Up Global API Key"},{"location":"tutorial/2024-05-19-apply-gemini/#windows-tutorial","text":"Run C:\\kuwa\\GenAI OS\\windows\\executors\\geminipro\\init.bat , and fill in the Google API key you just applied for. Restart Kuwa, or go back to the command line of Kuwa GenAI OS and input reload to reload all executors and start using Gemini.","title":"Windows Tutorial"},{"location":"tutorial/2024-05-19-apply-gemini/#docker-tutorial","text":"Refer to docker/compose/gemini.yaml to create a Gemini executor. Replace <YOUR_GLOBAL_API_KEY_HERE> with the Google API key you applied in the previous section. yaml services: gemini-executor: build: context: ../../ dockerfile: docker/executor/Dockerfile image: kuwa-executor environment: EXECUTOR_TYPE: geminipro EXECUTOR_ACCESS_CODE: geminipro EXECUTOR_NAME: Gemini depends_on: - kernel - multi-chat command: [\"--api_key\", \"<YOUR_GLOBAL_API_KEY_HERE>\"] restart: unless-stopped networks: [\"backend\"] 2. Add gemini to the confs array in docker/run.sh , and re-run docker/run.sh to start the Gemini executor.","title":"Docker Tutorial"},{"location":"tutorial/2024-05-19-dbqa-setup/","text":"v0.3.0 has added the RAG toolchain, which allows users to drag and drop their on-premises file folders to build their own vector database and do QnA. This article will guide you through how to use Kuwa's RAG toolchain to build your own vector database and related Bot. Windows Version Instruction Drag a single file or a file directory to the Create VectorDB shortcut on the desktop to open it with this script. If you don't have this shortcut, you can drag the file to C:\\kuwa\\GenAI OS\\windows\\construct_rag.bat The script will automatically create a vector database and related Bot, and if the screen appears as shown in the attached picture, it means the creation is successful Restart Kuwa or return to the command line of Kuwa GenAI OS and enter the reload command to reload all Executors After reloading, you can see a Bot with the same name as the file directory, and you can start QnA for the on-premises database Docker Version Instruction Refer to the document genai-os/src/toolchain/README.md to use the command to create vector database Refer to docker/compose/dbqa.yaml to create the DB QA Executor. Change the </path/to/vector-database> in the volume to the location of the vector database on the Host, EXECUTOR_NAME can be changed to an easy-to-remember name. The --model parameter can be used to specify a certain model to answer, if the --model parameter is omitted, the first online Executor in the Kernel will be selected (excluding Executors with the suffix \"-qa\") to answer. yaml services: dbqa-executor: build: context: ../../ dockerfile: docker/executor/Dockerfile image: kuwa-executor environment: CUSTOM_EXECUTOR_PATH: ./docqa/docqa.py EXECUTOR_ACCESS_CODE: db-qa EXECUTOR_NAME: DB QA volumes: [ \"</path/to/vector-database>:/var/database\" ] depends_on: - kernel - multi-chat command: [ \"--api_base_url\", \"http://web/\", \"--model\", \"geminipro\" \"--database\", \"/var/database\" ] restart: unless-stopped networks: [\"backend\", \"frontend\"] 3. Add dbqa to the confs array in docker/run.sh and then re-execute docker/run.sh to start the DB QA Executor 4. If you want to add more databases, duplicate docker/compose/dbqa.yaml , and modify the service name and volume location of the vector database","title":"2024 05 19 dbqa setup"},{"location":"tutorial/2024-05-19-dbqa-setup/#windows-version-instruction","text":"Drag a single file or a file directory to the Create VectorDB shortcut on the desktop to open it with this script. If you don't have this shortcut, you can drag the file to C:\\kuwa\\GenAI OS\\windows\\construct_rag.bat The script will automatically create a vector database and related Bot, and if the screen appears as shown in the attached picture, it means the creation is successful Restart Kuwa or return to the command line of Kuwa GenAI OS and enter the reload command to reload all Executors After reloading, you can see a Bot with the same name as the file directory, and you can start QnA for the on-premises database","title":"Windows Version Instruction"},{"location":"tutorial/2024-05-19-dbqa-setup/#docker-version-instruction","text":"Refer to the document genai-os/src/toolchain/README.md to use the command to create vector database Refer to docker/compose/dbqa.yaml to create the DB QA Executor. Change the </path/to/vector-database> in the volume to the location of the vector database on the Host, EXECUTOR_NAME can be changed to an easy-to-remember name. The --model parameter can be used to specify a certain model to answer, if the --model parameter is omitted, the first online Executor in the Kernel will be selected (excluding Executors with the suffix \"-qa\") to answer. yaml services: dbqa-executor: build: context: ../../ dockerfile: docker/executor/Dockerfile image: kuwa-executor environment: CUSTOM_EXECUTOR_PATH: ./docqa/docqa.py EXECUTOR_ACCESS_CODE: db-qa EXECUTOR_NAME: DB QA volumes: [ \"</path/to/vector-database>:/var/database\" ] depends_on: - kernel - multi-chat command: [ \"--api_base_url\", \"http://web/\", \"--model\", \"geminipro\" \"--database\", \"/var/database\" ] restart: unless-stopped networks: [\"backend\", \"frontend\"] 3. Add dbqa to the confs array in docker/run.sh and then re-execute docker/run.sh to start the DB QA Executor 4. If you want to add more databases, duplicate docker/compose/dbqa.yaml , and modify the service name and volume location of the vector database","title":"Docker Version Instruction"},{"location":"tutorial/2024-05-19-searchqa-setup/","text":"v0.3.0 added SearchQA, leveraging Google search to provide solutions for organizational QnA. This article will provide you steps on how to implement SearchQA. Google API Key Application [!WARNING] Google search API currently has a daily free quota of 100 times, please use it with caution Go to Google Programmable Search Engine Create Page and fill in the following information to create a custom search engine Click \"Customize\" after creating a new search engine to copy the CSE ID (Custom Search Engine ID) and API key The Search engine ID can be found under the Overview section, remember this ID, and use CSE ID to represent it later Scroll down the page and go to Custom Search JSON API Click the \"Get a Key\" button to get the API key You can use an existing Google cloud project or create a new one Click SHOW KEY to display the API key, remember this key, it will be used later Windows Tutorial Execute C:\\kuwa\\GenAI OS\\windows\\executors\\SearchQA\\init.bat , fill in the API key and CSE ID you just applied for, If you want to limit the search results to certain domains, you can fill in restricted sites ; if there are multiple domains, please separate them with a semicolon (;) Restart Kuwa, or go back to the command line of Kuwa GenAI OS, enter the command \"reload\" to reload all Executors You can see SearchQA after reloading, which can be used to answer questions based on information on the Internet Docker Tutorial Please refer to docker/compose/searchqa.yaml to create the SearchQA Executor. Replace <YOUR_GOOGLE_API_KEY> and <YOUR_GOOGLE_CUSTOM_SEARCH_ENGINE_ID> with the API key and CSE ID applied in the previous stage, EXECUTOR_NAME can be changed to an easy-to-remember name. --restricted_sites can restrict search results to certain domains; if there are multiple domains, please separate them with semicolons (;). The --model parameter can be used to specify a model to answer. If the --model parameter is omitted, the first Executor online in the Kernel (excluding Executors with \"-qa\" prefixes or suffixes) will be used to answer. yaml services: searchqa-executor: build: context: ../../ dockerfile: docker/executor/Dockerfile image: kuwa-executor environment: CUSTOM_EXECUTOR_PATH: ./docqa/searchqa.py EXECUTOR_ACCESS_CODE: search-qa EXECUTOR_NAME: SearchQA depends_on: - kernel - multi-chat command: [ \"--api_base_url\", \"http://web/\", \"--model\", \"geminipro\", \"--google_api_key\", \"<YOUR_GOOGLE_API_KEY>\", \"--google_cse_id\", \"<YOUR_GOOGLE_CUSTOM_SEARCH_ENGINE_ID>\", #\"--restricted_sites\", \"example.tw;example.com\" ] extra_hosts: - \"localhost:host-gateway\" restart: unless-stopped networks: [\"backend\", \"frontend\"] 2. Add searchqa to the confs array of docker/run.sh and then re-execute docker/run.sh to start the SearchQA Executor","title":"2024 05 19 searchqa setup"},{"location":"tutorial/2024-05-19-searchqa-setup/#google-api-key-application","text":"[!WARNING] Google search API currently has a daily free quota of 100 times, please use it with caution Go to Google Programmable Search Engine Create Page and fill in the following information to create a custom search engine Click \"Customize\" after creating a new search engine to copy the CSE ID (Custom Search Engine ID) and API key The Search engine ID can be found under the Overview section, remember this ID, and use CSE ID to represent it later Scroll down the page and go to Custom Search JSON API Click the \"Get a Key\" button to get the API key You can use an existing Google cloud project or create a new one Click SHOW KEY to display the API key, remember this key, it will be used later","title":"Google API Key Application"},{"location":"tutorial/2024-05-19-searchqa-setup/#windows-tutorial","text":"Execute C:\\kuwa\\GenAI OS\\windows\\executors\\SearchQA\\init.bat , fill in the API key and CSE ID you just applied for, If you want to limit the search results to certain domains, you can fill in restricted sites ; if there are multiple domains, please separate them with a semicolon (;) Restart Kuwa, or go back to the command line of Kuwa GenAI OS, enter the command \"reload\" to reload all Executors You can see SearchQA after reloading, which can be used to answer questions based on information on the Internet","title":"Windows Tutorial"},{"location":"tutorial/2024-05-19-searchqa-setup/#docker-tutorial","text":"Please refer to docker/compose/searchqa.yaml to create the SearchQA Executor. Replace <YOUR_GOOGLE_API_KEY> and <YOUR_GOOGLE_CUSTOM_SEARCH_ENGINE_ID> with the API key and CSE ID applied in the previous stage, EXECUTOR_NAME can be changed to an easy-to-remember name. --restricted_sites can restrict search results to certain domains; if there are multiple domains, please separate them with semicolons (;). The --model parameter can be used to specify a model to answer. If the --model parameter is omitted, the first Executor online in the Kernel (excluding Executors with \"-qa\" prefixes or suffixes) will be used to answer. yaml services: searchqa-executor: build: context: ../../ dockerfile: docker/executor/Dockerfile image: kuwa-executor environment: CUSTOM_EXECUTOR_PATH: ./docqa/searchqa.py EXECUTOR_ACCESS_CODE: search-qa EXECUTOR_NAME: SearchQA depends_on: - kernel - multi-chat command: [ \"--api_base_url\", \"http://web/\", \"--model\", \"geminipro\", \"--google_api_key\", \"<YOUR_GOOGLE_API_KEY>\", \"--google_cse_id\", \"<YOUR_GOOGLE_CUSTOM_SEARCH_ENGINE_ID>\", #\"--restricted_sites\", \"example.tw;example.com\" ] extra_hosts: - \"localhost:host-gateway\" restart: unless-stopped networks: [\"backend\", \"frontend\"] 2. Add searchqa to the confs array of docker/run.sh and then re-execute docker/run.sh to start the SearchQA Executor","title":"Docker Tutorial"},{"location":"tutorial/2024-06-24-painter-tutorial/","text":"Kuwa v0.3.1 adds Kuwa Painter based on the Stable Diffusion image generation model, You can generate an image by inputting a text, or upload an image and generate an image with a text. Known issues and limitations Hardware requirements The default model uses stable-diffusion-2, and the VRAM consumed when running on GPU is as shown in the following table. Model Name VRAM requirement stable-diffusion-2 ~3GB stable-diffusion-xl-base-1.0 ~8GB sdxl-turbo ~8GB stable-diffusion-3-medium-diffusers ~18 GB Known limitations sdxl-turbo throws an error while performing img2img Build Painter Executor Windows version startup steps The Windows version should automatically execute Painter Executor by default. If it is not executed, please follow the steps below: 1. Double-click C:\\kuwa\\GenAI OS\\windows\\executors\\painter\\init.bat to generate related execution settings 2. Restart Kuwa, or reload the Executor by inputting reload in the terminal window of Kuwa 3. An Executor named Painter should be added to your Kuwa system Docker version startup steps The Docker compose configuration file for Kuwa Speech Recognizer is located in docker/compose/painter.yaml . You can refer to the following steps to start it: 1. Add \"painter\" to the confs array in docker/run.sh (copy from run.sh.sample if the file does not exist) 2. Execute docker/run.sh up --build --remove-orphans --force-recreate 3. An Executor named Painter should be added to your Kuwa system Using Painter Text to Image You can input a text and let Kuwa Painter generate an image for you. It is important to note that the original Stable Diffusion model has a poor understanding of Chinese. At this time, you can use the group chat and quoting functions of Kuwa to let other language models translate the user's Prompt first, and then ask the Stable Diffusion model to generate an image, which usually gives better results. The first generated image in the figure below is based on the original Chinese User prompt ( \u96fb\u5f71\u98a8\u683c\u756b\u9762\u3002\u64c1\u6709\u96c4\u5049\u9e7f\u89d2\u7684\u96c4\u9e7f\uff0c\u5728\u7fe0\u7da0\u7684\u68ee\u6797\u88e1\u5b89\u975c\u5730\u4f4e\u982d\u5403\u8349\u3002 ), and the second image is the Prompt translated by TAIDE ( Film-inspired scene. A majestic stag with impressive antlers grazing serenely amidst a verdant forest. ) was used as the input to Stable Diffusion, and the quality difference between the two images is significant. Image to Image You can also upload a sketch, and then describe what you want to draw, and Kuwa Painter will draw it for you. Complete configuration instructions Kuwa Painter can adjust generation parameters through the Modelfile in the front-end Store. Commonly adjustable parameters are as follows PARAMETER model_name stabilityai/stable-diffusion-xl-base-1.0 PARAMETER imgen_num_inference_steps 40 # The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference PARAMETER imgen_negative_prompt \"\" #The prompt or prompts to guide what to not include in image generation. If not defined, you need to pass negative_prompt_embeds instead. Ignored when not using guidance (guidance_scale < 1). PARAMETER imgen_strength 0.5 #Indicates extent to transform the reference image. Must be between 0 and 1. image is used as a starting point and more noise is added the higher the strength. The number of denoising steps depends on the amount of noise initially added. When strength is 1, added noise is maximum and the denoising process runs for the full number of iterations specified in num_inference_steps. A value of 1 essentially ignores image. PARAMETER imgen_guidance_scale 0.0 #A higher guidance scale value encourages the model to generate images closely linked to the text prompt at the expense of lower image quality. Guidance scale is enabled when guidance_scale PARAMETER imgen_denoising_end 0.8 # What % of steps to be run on each experts (80/20) (SDXL only) In addition, Kuwa Painter can also be set through dynamic command line parameters. The parameters that can be set are as follows Model Options: --model MODEL The name of the stable diffusion model to use. (default: stabilityai/stable-diffusion-2) --n_cache N_CACHE How many models to cache. (default: 3) Display Options: --show_progress Whether to show the progress of generation. (default: False) For more information, please refer to genai-os/src/executor/image_generation/README.md","title":"2024 06 24 painter tutorial"},{"location":"tutorial/2024-06-24-painter-tutorial/#known-issues-and-limitations","text":"","title":"Known issues and limitations"},{"location":"tutorial/2024-06-24-painter-tutorial/#hardware-requirements","text":"The default model uses stable-diffusion-2, and the VRAM consumed when running on GPU is as shown in the following table. Model Name VRAM requirement stable-diffusion-2 ~3GB stable-diffusion-xl-base-1.0 ~8GB sdxl-turbo ~8GB stable-diffusion-3-medium-diffusers ~18 GB","title":"Hardware requirements"},{"location":"tutorial/2024-06-24-painter-tutorial/#known-limitations","text":"sdxl-turbo throws an error while performing img2img","title":"Known limitations"},{"location":"tutorial/2024-06-24-painter-tutorial/#build-painter-executor","text":"","title":"Build Painter Executor"},{"location":"tutorial/2024-06-24-painter-tutorial/#windows-version-startup-steps","text":"The Windows version should automatically execute Painter Executor by default. If it is not executed, please follow the steps below: 1. Double-click C:\\kuwa\\GenAI OS\\windows\\executors\\painter\\init.bat to generate related execution settings 2. Restart Kuwa, or reload the Executor by inputting reload in the terminal window of Kuwa 3. An Executor named Painter should be added to your Kuwa system","title":"Windows version startup steps"},{"location":"tutorial/2024-06-24-painter-tutorial/#docker-version-startup-steps","text":"The Docker compose configuration file for Kuwa Speech Recognizer is located in docker/compose/painter.yaml . You can refer to the following steps to start it: 1. Add \"painter\" to the confs array in docker/run.sh (copy from run.sh.sample if the file does not exist) 2. Execute docker/run.sh up --build --remove-orphans --force-recreate 3. An Executor named Painter should be added to your Kuwa system","title":"Docker version startup steps"},{"location":"tutorial/2024-06-24-painter-tutorial/#using-painter","text":"","title":"Using Painter"},{"location":"tutorial/2024-06-24-painter-tutorial/#text-to-image","text":"You can input a text and let Kuwa Painter generate an image for you. It is important to note that the original Stable Diffusion model has a poor understanding of Chinese. At this time, you can use the group chat and quoting functions of Kuwa to let other language models translate the user's Prompt first, and then ask the Stable Diffusion model to generate an image, which usually gives better results. The first generated image in the figure below is based on the original Chinese User prompt ( \u96fb\u5f71\u98a8\u683c\u756b\u9762\u3002\u64c1\u6709\u96c4\u5049\u9e7f\u89d2\u7684\u96c4\u9e7f\uff0c\u5728\u7fe0\u7da0\u7684\u68ee\u6797\u88e1\u5b89\u975c\u5730\u4f4e\u982d\u5403\u8349\u3002 ), and the second image is the Prompt translated by TAIDE ( Film-inspired scene. A majestic stag with impressive antlers grazing serenely amidst a verdant forest. ) was used as the input to Stable Diffusion, and the quality difference between the two images is significant.","title":"Text to Image"},{"location":"tutorial/2024-06-24-painter-tutorial/#image-to-image","text":"You can also upload a sketch, and then describe what you want to draw, and Kuwa Painter will draw it for you.","title":"Image to Image"},{"location":"tutorial/2024-06-24-painter-tutorial/#complete-configuration-instructions","text":"Kuwa Painter can adjust generation parameters through the Modelfile in the front-end Store. Commonly adjustable parameters are as follows PARAMETER model_name stabilityai/stable-diffusion-xl-base-1.0 PARAMETER imgen_num_inference_steps 40 # The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference PARAMETER imgen_negative_prompt \"\" #The prompt or prompts to guide what to not include in image generation. If not defined, you need to pass negative_prompt_embeds instead. Ignored when not using guidance (guidance_scale < 1). PARAMETER imgen_strength 0.5 #Indicates extent to transform the reference image. Must be between 0 and 1. image is used as a starting point and more noise is added the higher the strength. The number of denoising steps depends on the amount of noise initially added. When strength is 1, added noise is maximum and the denoising process runs for the full number of iterations specified in num_inference_steps. A value of 1 essentially ignores image. PARAMETER imgen_guidance_scale 0.0 #A higher guidance scale value encourages the model to generate images closely linked to the text prompt at the expense of lower image quality. Guidance scale is enabled when guidance_scale PARAMETER imgen_denoising_end 0.8 # What % of steps to be run on each experts (80/20) (SDXL only) In addition, Kuwa Painter can also be set through dynamic command line parameters. The parameters that can be set are as follows Model Options: --model MODEL The name of the stable diffusion model to use. (default: stabilityai/stable-diffusion-2) --n_cache N_CACHE How many models to cache. (default: 3) Display Options: --show_progress Whether to show the progress of generation. (default: False) For more information, please refer to genai-os/src/executor/image_generation/README.md","title":"Complete configuration instructions"},{"location":"tutorial/2024-06-24-vlm-tutorial/","text":"Kuwa v0.3.1 has preliminary support for commonly used visual language models (VLMs). In addition to text inputs, such models can also take images as input and respond to user instructions based on the content of the images. This tutorial will guide you through the initial setup and usage of VLMs. VLM Executor Setup Kuwa v0.3.1 extends the original Huggingface executor to support VLMs. Currently, three common VLMs are preliminarily supported: Phi-3-Vision, LLaVA v1.5, and LLaVA v1.6. The following uses LLaVA v1.6 as an example. Windows Build Procedure Refer to the previous Llama3 build tutorial , enter llava-hf/llava-v1.6-mistral-7b-hf for \"model path\", and leave \"Arguments to use\" blank. Docker Build Procedure Refer to the following Docker compose settings: services: llava-v1.6-executor: image: kuwaai/model-executor environment: EXECUTOR_TYPE: huggingface EXECUTOR_ACCESS_CODE: llava-v1.6-7b EXECUTOR_NAME: LLaVA v1.6 7B # HUGGING_FACE_HUB_TOKEN: ${HUGGING_FACE_HUB_TOKEN} depends_on: - executor-builder - kernel - multi-chat command: [\"--model_path\", \"llava-hf/llava-v1.6-mistral-7b-hf\", \"--log\", \"debug\"] restart: unless-stopped volumes: [\"~/.cache/huggingface:/root/.cache/huggingface\"] deploy: resources: reservations: devices: - driver: nvidia device_ids: ['0'] capabilities: [gpu] networks: [\"backend\"] VLM Usage You can upload an image and ask questions about it, or ask the model to identify the text on it.","title":"2024 06 24 vlm tutorial"},{"location":"tutorial/2024-06-24-vlm-tutorial/#vlm-executor-setup","text":"Kuwa v0.3.1 extends the original Huggingface executor to support VLMs. Currently, three common VLMs are preliminarily supported: Phi-3-Vision, LLaVA v1.5, and LLaVA v1.6. The following uses LLaVA v1.6 as an example.","title":"VLM Executor Setup"},{"location":"tutorial/2024-06-24-vlm-tutorial/#windows-build-procedure","text":"Refer to the previous Llama3 build tutorial , enter llava-hf/llava-v1.6-mistral-7b-hf for \"model path\", and leave \"Arguments to use\" blank.","title":"Windows Build Procedure"},{"location":"tutorial/2024-06-24-vlm-tutorial/#docker-build-procedure","text":"Refer to the following Docker compose settings: services: llava-v1.6-executor: image: kuwaai/model-executor environment: EXECUTOR_TYPE: huggingface EXECUTOR_ACCESS_CODE: llava-v1.6-7b EXECUTOR_NAME: LLaVA v1.6 7B # HUGGING_FACE_HUB_TOKEN: ${HUGGING_FACE_HUB_TOKEN} depends_on: - executor-builder - kernel - multi-chat command: [\"--model_path\", \"llava-hf/llava-v1.6-mistral-7b-hf\", \"--log\", \"debug\"] restart: unless-stopped volumes: [\"~/.cache/huggingface:/root/.cache/huggingface\"] deploy: resources: reservations: devices: - driver: nvidia device_ids: ['0'] capabilities: [gpu] networks: [\"backend\"]","title":"Docker Build Procedure"},{"location":"tutorial/2024-06-24-vlm-tutorial/#vlm-usage","text":"You can upload an image and ask questions about it, or ask the model to identify the text on it.","title":"VLM Usage"},{"location":"tutorial/2024-06-24-whisper-tutorial/","text":"Kuwa v0.3.1 adds Kuwa Speech Recognizer based on the Whisper speech recognition model, which can generate transcripts by uploading audio files, supporting timestamps and speaker labels. Known Issues and Limitations Hardware requirements The default Whisper medium model is used with speaker diarization disabled. The VRAM consumption on GPU is shown in the following table. Model Name Number of parameters VRAM requirement Relative recognition speed tiny 39 M ~1 GB ~32x base 74 M ~1 GB ~16x small 244 M ~2 GB ~6x medium 769 M ~5 GB ~2x large 1550 M ~10 GB 1x pyannote/speaker-diarization-3.1 (Speaker Diarization) - ~3GB - Known limitations Currently, the input language cannot be detected automatically and must be specified manually. Currently, the speaker identification module is multi-threaded, causing the model to be reloaded each time, resulting in a longer response time. Content is easily misjudged when multiple speakers speak at the same time. Build Whisper Executor Prerequisites If you need speaker identification, please follow the steps below to obtain the authorization for the speaker identification model: Agree to the pyannote/segmentation-3.0 and pyannote/speaker-diarization-3.1 license terms. Refer to the instructions for each version to add the HuggingFace access token. Guide for Windows Guide for Docker Windows Launch Steps The Whisper Executor should run automatically by default on Windows. If it is not running, follow these steps: 1. Double click C:\\kuwa\\GenAI OS\\windows\\executors\\download.bat to download Whisper model. If you need speech recognition, you can also download Diarization Model at the same time. 2. Double-click C:\\kuwa\\GenAI OS\\windows\\executors\\whisper\\init.bat to generate the relevant execution settings. 3. Restart Kuwa, or type reload in Kuwa's terminal window to reload the Executor. 4. An Executor named Whisper should be added to your Kuwa system. Docker Launch Steps The Docker compose configuration file for the Kuwa Speech Recognizer is located at docker/compose/whisper.yaml . You can refer to the following steps to start it: 1. Add \"whisper\" to the confs array in docker/run.sh (copy from run.sh.sample if the file does not exist). 2. Execute docker/run.sh up --build --remove-orphans --force-recreate . 3. An Executor named Whisper should be added to your Kuwa system. Using Whisper Speech to Text You can upload an audio file to generate a transcript. The default recognition language is English. Create a Bot and add the parameter PARAMETER whisper_language zh to generate transcripts in Chinese or other languages. The Whisper model for Chinese does not output punctuation marks by default. You can influence the model's output using the User prompt or System prompt. Transcript Timestamps Add the parameter PARAMETER whisper_enable_timestamp True to the Bot configuration file to enable timestamps for the transcript. :::info In the above example, the user enters \".\" just to access the previously uploaded audio file. Remember to select the chat mode as \"Continuous Q&A\". ::: Speaker Diarization Similarly, add the parameter PARAMETER whisper_enable_diarization True to the Bot configuration file to enable speaker identification and labeling. [!Note] The command /replace <pattern> <repl> allows you to replace the recognition results using regular expressions. You can replace speaker names or repeatedly misidentified words. The command /speakers <n> allows you to specify the number of speakers. The default is to detect automatically, but it may be biased. You can use this command to correct it. Full Configuration Description The following parameters can be set for Bots. For a complete description, please refer to ganai-os/src/executor/speech_recognition/README.md . SYSTEM \"Add punctuation.\" #Custom vocabulary or prompting PARAMETER whisper_model medium #Model name. Choses: large-v1, large-v2, large-v3, medium, base, small, tiny PARAMETER whisper_enable_timestamp True #Prepend the text a timestamp PARAMETER whisper_enable_diarization True #Label the speaker PARAMETER whisper_diar_thold_sec 2 #Time before speakers are tagged in paragraphs that are longer than. (in seconds) PARAMETER whisper_language en #The language of the audio PARAMETER whisper_n_threads None #Number of threads to allocate for the inference. default to min(4, available hardware_concurrency) PARAMETER whisper_n_max_text_ctx 16384 #max tokens to use from past text as prompt for the decoder PARAMETER whisper_offset_ms 0 #start offset in ms PARAMETER whisper_duration_ms 0 #audio duration to process in ms PARAMETER whisper_translate False #whether to translate the audio to English PARAMETER whisper_no_context False #do not use past transcription (if any) as initial prompt for the decoder PARAMETER whisper_single_segment False #force single segment output (useful for streaming) PARAMETER whisper_print_special False #print special tokens (e.g. <SOT>, <EOT>, <BEG>, etc.) PARAMETER whisper_print_progress True #print progress information PARAMETER whisper_print_realtime False #print results from within whisper.cpp (avoid it, use callback instead) PARAMETER whisper_print_timestamps True #print timestamps for each text segment when printing realtime PARAMETER whisper_token_timestamps False #enable token-level timestamps PARAMETER whisper_thold_pt 0.01 #timestamp token probability threshold (~0.01) PARAMETER whisper_thold_ptsum 0.01 #timestamp token sum probability threshold (~0.01) PARAMETER whisper_max_len 0 #max segment length in characters PARAMETER whisper_split_on_word False #split on word rather than on token (when used with max_len) PARAMETER whisper_max_tokens 0 #max tokens per segment (0 = no limit) PARAMETER whisper_speed_up False #speed-up the audio by 2x using Phase Vocoder PARAMETER whisper_audio_ctx 0 #overwrite the audio context size (0 = use default) PARAMETER whisper_initial_prompt None #Initial prompt, these are prepended to any existing text context from a previous call PARAMETER whisper_prompt_tokens None #tokens to provide to the whisper decoder as initial prompt PARAMETER whisper_prompt_n_tokens 0 #tokens to provide to the whisper decoder as initial prompt PARAMETER whisper_suppress_blank True #common decoding parameters PARAMETER whisper_suppress_non_speech_tokens False #common decoding parameters PARAMETER whisper_temperature 0.0 #initial decoding temperature PARAMETER whisper_max_initial_ts 1.0 #max_initial_ts PARAMETER whisper_length_penalty -1.0 #length_penalty PARAMETER whisper_temperature_inc 0.2 #temperature_inc PARAMETER whisper_entropy_thold 2.4 #similar to OpenAI's \"compression_ratio_threshold\" PARAMETER whisper_logprob_thold -1.0 #logprob_thold PARAMETER whisper_no_speech_thold 0.6 #no_speech_thold","title":"2024 06 24 whisper tutorial"},{"location":"tutorial/2024-06-24-whisper-tutorial/#known-issues-and-limitations","text":"","title":"Known Issues and Limitations"},{"location":"tutorial/2024-06-24-whisper-tutorial/#hardware-requirements","text":"The default Whisper medium model is used with speaker diarization disabled. The VRAM consumption on GPU is shown in the following table. Model Name Number of parameters VRAM requirement Relative recognition speed tiny 39 M ~1 GB ~32x base 74 M ~1 GB ~16x small 244 M ~2 GB ~6x medium 769 M ~5 GB ~2x large 1550 M ~10 GB 1x pyannote/speaker-diarization-3.1 (Speaker Diarization) - ~3GB -","title":"Hardware requirements"},{"location":"tutorial/2024-06-24-whisper-tutorial/#known-limitations","text":"Currently, the input language cannot be detected automatically and must be specified manually. Currently, the speaker identification module is multi-threaded, causing the model to be reloaded each time, resulting in a longer response time. Content is easily misjudged when multiple speakers speak at the same time.","title":"Known limitations"},{"location":"tutorial/2024-06-24-whisper-tutorial/#build-whisper-executor","text":"","title":"Build Whisper Executor"},{"location":"tutorial/2024-06-24-whisper-tutorial/#prerequisites","text":"If you need speaker identification, please follow the steps below to obtain the authorization for the speaker identification model: Agree to the pyannote/segmentation-3.0 and pyannote/speaker-diarization-3.1 license terms. Refer to the instructions for each version to add the HuggingFace access token. Guide for Windows Guide for Docker","title":"Prerequisites"},{"location":"tutorial/2024-06-24-whisper-tutorial/#windows-launch-steps","text":"The Whisper Executor should run automatically by default on Windows. If it is not running, follow these steps: 1. Double click C:\\kuwa\\GenAI OS\\windows\\executors\\download.bat to download Whisper model. If you need speech recognition, you can also download Diarization Model at the same time. 2. Double-click C:\\kuwa\\GenAI OS\\windows\\executors\\whisper\\init.bat to generate the relevant execution settings. 3. Restart Kuwa, or type reload in Kuwa's terminal window to reload the Executor. 4. An Executor named Whisper should be added to your Kuwa system.","title":"Windows Launch Steps"},{"location":"tutorial/2024-06-24-whisper-tutorial/#docker-launch-steps","text":"The Docker compose configuration file for the Kuwa Speech Recognizer is located at docker/compose/whisper.yaml . You can refer to the following steps to start it: 1. Add \"whisper\" to the confs array in docker/run.sh (copy from run.sh.sample if the file does not exist). 2. Execute docker/run.sh up --build --remove-orphans --force-recreate . 3. An Executor named Whisper should be added to your Kuwa system.","title":"Docker Launch Steps"},{"location":"tutorial/2024-06-24-whisper-tutorial/#using-whisper","text":"","title":"Using Whisper"},{"location":"tutorial/2024-06-24-whisper-tutorial/#speech-to-text","text":"You can upload an audio file to generate a transcript. The default recognition language is English. Create a Bot and add the parameter PARAMETER whisper_language zh to generate transcripts in Chinese or other languages. The Whisper model for Chinese does not output punctuation marks by default. You can influence the model's output using the User prompt or System prompt.","title":"Speech to Text"},{"location":"tutorial/2024-06-24-whisper-tutorial/#transcript-timestamps","text":"Add the parameter PARAMETER whisper_enable_timestamp True to the Bot configuration file to enable timestamps for the transcript. :::info In the above example, the user enters \".\" just to access the previously uploaded audio file. Remember to select the chat mode as \"Continuous Q&A\". :::","title":"Transcript Timestamps"},{"location":"tutorial/2024-06-24-whisper-tutorial/#speaker-diarization","text":"Similarly, add the parameter PARAMETER whisper_enable_diarization True to the Bot configuration file to enable speaker identification and labeling. [!Note] The command /replace <pattern> <repl> allows you to replace the recognition results using regular expressions. You can replace speaker names or repeatedly misidentified words. The command /speakers <n> allows you to specify the number of speakers. The default is to detect automatically, but it may be biased. You can use this command to correct it.","title":"Speaker Diarization"},{"location":"tutorial/2024-06-24-whisper-tutorial/#full-configuration-description","text":"The following parameters can be set for Bots. For a complete description, please refer to ganai-os/src/executor/speech_recognition/README.md . SYSTEM \"Add punctuation.\" #Custom vocabulary or prompting PARAMETER whisper_model medium #Model name. Choses: large-v1, large-v2, large-v3, medium, base, small, tiny PARAMETER whisper_enable_timestamp True #Prepend the text a timestamp PARAMETER whisper_enable_diarization True #Label the speaker PARAMETER whisper_diar_thold_sec 2 #Time before speakers are tagged in paragraphs that are longer than. (in seconds) PARAMETER whisper_language en #The language of the audio PARAMETER whisper_n_threads None #Number of threads to allocate for the inference. default to min(4, available hardware_concurrency) PARAMETER whisper_n_max_text_ctx 16384 #max tokens to use from past text as prompt for the decoder PARAMETER whisper_offset_ms 0 #start offset in ms PARAMETER whisper_duration_ms 0 #audio duration to process in ms PARAMETER whisper_translate False #whether to translate the audio to English PARAMETER whisper_no_context False #do not use past transcription (if any) as initial prompt for the decoder PARAMETER whisper_single_segment False #force single segment output (useful for streaming) PARAMETER whisper_print_special False #print special tokens (e.g. <SOT>, <EOT>, <BEG>, etc.) PARAMETER whisper_print_progress True #print progress information PARAMETER whisper_print_realtime False #print results from within whisper.cpp (avoid it, use callback instead) PARAMETER whisper_print_timestamps True #print timestamps for each text segment when printing realtime PARAMETER whisper_token_timestamps False #enable token-level timestamps PARAMETER whisper_thold_pt 0.01 #timestamp token probability threshold (~0.01) PARAMETER whisper_thold_ptsum 0.01 #timestamp token sum probability threshold (~0.01) PARAMETER whisper_max_len 0 #max segment length in characters PARAMETER whisper_split_on_word False #split on word rather than on token (when used with max_len) PARAMETER whisper_max_tokens 0 #max tokens per segment (0 = no limit) PARAMETER whisper_speed_up False #speed-up the audio by 2x using Phase Vocoder PARAMETER whisper_audio_ctx 0 #overwrite the audio context size (0 = use default) PARAMETER whisper_initial_prompt None #Initial prompt, these are prepended to any existing text context from a previous call PARAMETER whisper_prompt_tokens None #tokens to provide to the whisper decoder as initial prompt PARAMETER whisper_prompt_n_tokens 0 #tokens to provide to the whisper decoder as initial prompt PARAMETER whisper_suppress_blank True #common decoding parameters PARAMETER whisper_suppress_non_speech_tokens False #common decoding parameters PARAMETER whisper_temperature 0.0 #initial decoding temperature PARAMETER whisper_max_initial_ts 1.0 #max_initial_ts PARAMETER whisper_length_penalty -1.0 #length_penalty PARAMETER whisper_temperature_inc 0.2 #temperature_inc PARAMETER whisper_entropy_thold 2.4 #similar to OpenAI's \"compression_ratio_threshold\" PARAMETER whisper_logprob_thold -1.0 #logprob_thold PARAMETER whisper_no_speech_thold 0.6 #no_speech_thold","title":"Full Configuration Description"},{"location":"tutorial/2024-07-05-rag-param-tutorial/","text":"Kuwa's RAG application (DocQA/WebQA/DatabaseQA/SearchQA) supports customization of advanced parameters through the Bot's model file starting from version v0.3.1, allowing a single Executor to be virtualized into multiple RAG applications. Detailed parameter descriptions and examples are as follows. Parameter Description The following parameter contents are the default values for the v0.3.1 RAG application. Shared Parameters for All RAGs PARAMETER retriever_embedding_model \"thenlper/gte-base-zh\" # Embedding model name PARAMETER retriever_mmr_fetch_k 12 # MMR fetch k chunks PARAMETER retriever_mmr_k 6 # MMR fetch k chunks PARAMETER retriever_chunk_size 512 # Length of each chunk in characters (not restricted for DatabaseQA) PARAMETER retriever_chunk_overlap 128 # Overlap length between chunks in characters (not restricted for DatabaseQA) PARAMETER generator_model None # Specify which model to answer, None means auto-selection PARAMETER generator_limit 3072 # Length limit of the entire prompt in characters PARAMETER display_hide_ref False # Do not show references DocQA, WebQA, SearchQA Specific Parameters PARAMETER crawler_user_agent \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36\" # Crawler UA string SearchQA Specific Parameters PARAMETER search_advanced_params \"\" # Advanced search parameters (SearchQA only) PARAMETER search_num_url 3 # Number of search results to retrieve [1~10] (SearchQA only) DatabaseQA Specific Parameters PARAMETER retriever_database None # Path to vector database on local Executor Usage Example Suppose you want to create a DatabaseQA knowledge base and specify a model to answer, you can create a Bot, select DocQA as the base model, and fill in the following Modelfile. PARAMETER generator_model \"model_access_code\" # Specify which model to answer, None means auto-selection PARAMETER generator_limit 3072 # Length limit of the entire prompt in characters PARAMETER retriever_database \"/path/to/local/database/on/executor\" # Path to vector database on local Executor","title":"2024 07 05 rag param tutorial"},{"location":"tutorial/2024-07-05-rag-param-tutorial/#parameter-description","text":"The following parameter contents are the default values for the v0.3.1 RAG application.","title":"Parameter Description"},{"location":"tutorial/2024-07-05-rag-param-tutorial/#shared-parameters-for-all-rags","text":"PARAMETER retriever_embedding_model \"thenlper/gte-base-zh\" # Embedding model name PARAMETER retriever_mmr_fetch_k 12 # MMR fetch k chunks PARAMETER retriever_mmr_k 6 # MMR fetch k chunks PARAMETER retriever_chunk_size 512 # Length of each chunk in characters (not restricted for DatabaseQA) PARAMETER retriever_chunk_overlap 128 # Overlap length between chunks in characters (not restricted for DatabaseQA) PARAMETER generator_model None # Specify which model to answer, None means auto-selection PARAMETER generator_limit 3072 # Length limit of the entire prompt in characters PARAMETER display_hide_ref False # Do not show references","title":"Shared Parameters for All RAGs"},{"location":"tutorial/2024-07-05-rag-param-tutorial/#docqa-webqa-searchqa-specific-parameters","text":"PARAMETER crawler_user_agent \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36\" # Crawler UA string","title":"DocQA, WebQA, SearchQA Specific Parameters"},{"location":"tutorial/2024-07-05-rag-param-tutorial/#searchqa-specific-parameters","text":"PARAMETER search_advanced_params \"\" # Advanced search parameters (SearchQA only) PARAMETER search_num_url 3 # Number of search results to retrieve [1~10] (SearchQA only)","title":"SearchQA Specific Parameters"},{"location":"tutorial/2024-07-05-rag-param-tutorial/#databaseqa-specific-parameters","text":"PARAMETER retriever_database None # Path to vector database on local Executor","title":"DatabaseQA Specific Parameters"},{"location":"tutorial/2024-07-05-rag-param-tutorial/#usage-example","text":"Suppose you want to create a DatabaseQA knowledge base and specify a model to answer, you can create a Bot, select DocQA as the base model, and fill in the following Modelfile. PARAMETER generator_model \"model_access_code\" # Specify which model to answer, None means auto-selection PARAMETER generator_limit 3072 # Length limit of the entire prompt in characters PARAMETER retriever_database \"/path/to/local/database/on/executor\" # Path to vector database on local Executor","title":"Usage Example"},{"location":"tutorial/2024-07-05-tool-tutorial/","text":"Kuwa is designed to support the connection of various non-LLM tools. The simplest tool can refer to src/executor/debug.py . The following is a content description. import os import sys import asyncio import logging import json sys.path.append(os.path.dirname(os.path.abspath(__file__))) from kuwa.executor import LLMExecutor, Modelfile logger = logging.getLogger(__name__) class DebugExecutor(LLMExecutor): def __init__(self): super().__init__() def extend_arguments(self, parser): \"\"\" Override this method to add custom command-line arguments. \"\"\" parser.add_argument('--delay', type=float, default=0.02, help='Inter-token delay') def setup(self): self.stop = False async def llm_compute(self, history: list[dict], modelfile:Modelfile): \"\"\" Responsible for handling the requests, the input is chat history (in OpenAI format) and parsed Modelfile (you can refer to `genai-os/src/executor/src/kuwa/executor/modelfile.py`), it will return an Asynchronous Generators to represent the output stream. \"\"\" try: self.stop = False for i in \"\".join([i['content'] for i in history]).strip(): yield i if self.stop: self.stop = False break await asyncio.sleep(modelfile.parameters.get(\"llm_delay\", self.args.delay)) except Exception as e: logger.exception(\"Error occurs during generation.\") yield str(e) finally: logger.debug(\"finished\") async def abort(self): \"\"\" This method is invoked when the user presses the interrupt generation button. \"\"\" self.stop = True logger.debug(\"aborted\") return \"Aborted\" if __name__ == \"__main__\": executor = DebugExecutor() executor.run()","title":"2024 07 05 tool tutorial"},{"location":"tutorial/2024-07-18-cool-whisper-tutorial/","text":"National Taiwan University's Liang-Hsuan Tseng and NTU COOL team released the Cool-Whisper model last night (7/17), which is suitable for recognizing Taiwanese pronunciation Chinese or mixed Chinese-English audio files. Kuwa can directly apply it by simply modifying the Modelfile. [!Note] The model was temporarily taken offline around 12:00 on 7/18 due to privacy concerns. Friends who want to use this model can continue to follow its HuggingFace Hub and use it once it is re-released. Setup Steps Refer to the Whisper setup tutorial to start the Whisper executor The Cool-Whisper model is approximately 1.5 GB in size and will occupy up to 10 GB of VRAM during execution Create a new bot named Cool-Whisper in the store, select Whisper as the base model, and fill in the following model settings file, focusing on the PARAMETER whisper_model andybi7676/cool-whisper parameter dockerfile SYSTEM \"\u52a0\u5165\u6a19\u9ede\u7b26\u865f\u3002\" PARAMETER whisper_model andybi7676/cool-whisper #base, large-v1, large-v2, large-v3, medium, small, tiny PARAMETER whisper_enable_timestamp True #Do not prepend the text a timestamp PARAMETER whisper_enable_diarization False PARAMETER whisper_diar_thold_sec 2 PARAMETER whisper_language zh #for auto-detection, set to None, \"\" or \"auto\" PARAMETER whisper_n_threads None #Number of threads to allocate for the inferencedefault to min(4, available hardware_concurrency) PARAMETER whisper_n_max_text_ctx 16384 #max tokens to use from past text as prompt for the decoder PARAMETER whisper_offset_ms 0 #start offset in ms PARAMETER whisper_duration_ms 0 #audio duration to process in ms PARAMETER whisper_translate False #whether to translate the audio to English PARAMETER whisper_no_context False #do not use past transcription (if any) as initial prompt for the decoder PARAMETER whisper_single_segment False #force single segment output (useful for streaming) PARAMETER whisper_print_special False #print special tokens (e.g. <SOT>, <EOT>, <BEG>, etc.) PARAMETER whisper_print_progress True #print progress information PARAMETER whisper_print_realtime False #print results from within whisper.cpp (avoid it, use callback instead) PARAMETER whisper_print_timestamps True #print timestamps for each text segment when printing realtime PARAMETER whisper_token_timestamps False #enable token-level timestamps PARAMETER whisper_thold_pt 0.01 #timestamp token probability threshold (~0.01) PARAMETER whisper_thold_ptsum 0.01 #timestamp token sum probability threshold (~0.01) PARAMETER whisper_max_len 0 #max segment length in characters PARAMETER whisper_split_on_word False #split on word rather than on token (when used with max_len) PARAMETER whisper_max_tokens 0 #max tokens per segment (0 = no limit) PARAMETER whisper_speed_up False #speed-up the audio by 2x using Phase Vocoder PARAMETER whisper_audio_ctx 0 #overwrite the audio context size (0 = use default) PARAMETER whisper_initial_prompt None #Initial prompt, these are prepended to any existing text context from a previous call PARAMETER whisper_prompt_tokens None #tokens to provide to the whisper decoder as initial prompt PARAMETER whisper_prompt_n_tokens 0 #tokens to provide to the whisper decoder as initial prompt PARAMETER whisper_suppress_blank True #common decoding parameters PARAMETER whisper_suppress_non_speech_tokens False #common decoding parameters PARAMETER whisper_temperature 0.0 #initial decoding temperature PARAMETER whisper_max_initial_ts 1.0 #max_initial_ts PARAMETER whisper_length_penalty -1.0 #length_penalty PARAMETER whisper_temperature_inc 0.2 #temperature_inc PARAMETER whisper_entropy_thold 2.4 #similar to OpenAI's \"compression_ratio_threshold\" PARAMETER whisper_logprob_thold -1.0 #logprob_thold PARAMETER whisper_no_speech_thold 0.6 #no_speech_thold You can now use the Cool-Whisper model for speech recognition. The following figure shows the use of Whisper and Cool-Whisper for recognizing mixed Chinese-English audio files, which can accurately recognize mixed Chinese-English scenarios References Cool-Whisper's HuggingFace Hub Professor Lee's Facebook post","title":"2024 07 18 cool whisper tutorial"},{"location":"tutorial/2024-07-18-cool-whisper-tutorial/#setup-steps","text":"Refer to the Whisper setup tutorial to start the Whisper executor The Cool-Whisper model is approximately 1.5 GB in size and will occupy up to 10 GB of VRAM during execution Create a new bot named Cool-Whisper in the store, select Whisper as the base model, and fill in the following model settings file, focusing on the PARAMETER whisper_model andybi7676/cool-whisper parameter dockerfile SYSTEM \"\u52a0\u5165\u6a19\u9ede\u7b26\u865f\u3002\" PARAMETER whisper_model andybi7676/cool-whisper #base, large-v1, large-v2, large-v3, medium, small, tiny PARAMETER whisper_enable_timestamp True #Do not prepend the text a timestamp PARAMETER whisper_enable_diarization False PARAMETER whisper_diar_thold_sec 2 PARAMETER whisper_language zh #for auto-detection, set to None, \"\" or \"auto\" PARAMETER whisper_n_threads None #Number of threads to allocate for the inferencedefault to min(4, available hardware_concurrency) PARAMETER whisper_n_max_text_ctx 16384 #max tokens to use from past text as prompt for the decoder PARAMETER whisper_offset_ms 0 #start offset in ms PARAMETER whisper_duration_ms 0 #audio duration to process in ms PARAMETER whisper_translate False #whether to translate the audio to English PARAMETER whisper_no_context False #do not use past transcription (if any) as initial prompt for the decoder PARAMETER whisper_single_segment False #force single segment output (useful for streaming) PARAMETER whisper_print_special False #print special tokens (e.g. <SOT>, <EOT>, <BEG>, etc.) PARAMETER whisper_print_progress True #print progress information PARAMETER whisper_print_realtime False #print results from within whisper.cpp (avoid it, use callback instead) PARAMETER whisper_print_timestamps True #print timestamps for each text segment when printing realtime PARAMETER whisper_token_timestamps False #enable token-level timestamps PARAMETER whisper_thold_pt 0.01 #timestamp token probability threshold (~0.01) PARAMETER whisper_thold_ptsum 0.01 #timestamp token sum probability threshold (~0.01) PARAMETER whisper_max_len 0 #max segment length in characters PARAMETER whisper_split_on_word False #split on word rather than on token (when used with max_len) PARAMETER whisper_max_tokens 0 #max tokens per segment (0 = no limit) PARAMETER whisper_speed_up False #speed-up the audio by 2x using Phase Vocoder PARAMETER whisper_audio_ctx 0 #overwrite the audio context size (0 = use default) PARAMETER whisper_initial_prompt None #Initial prompt, these are prepended to any existing text context from a previous call PARAMETER whisper_prompt_tokens None #tokens to provide to the whisper decoder as initial prompt PARAMETER whisper_prompt_n_tokens 0 #tokens to provide to the whisper decoder as initial prompt PARAMETER whisper_suppress_blank True #common decoding parameters PARAMETER whisper_suppress_non_speech_tokens False #common decoding parameters PARAMETER whisper_temperature 0.0 #initial decoding temperature PARAMETER whisper_max_initial_ts 1.0 #max_initial_ts PARAMETER whisper_length_penalty -1.0 #length_penalty PARAMETER whisper_temperature_inc 0.2 #temperature_inc PARAMETER whisper_entropy_thold 2.4 #similar to OpenAI's \"compression_ratio_threshold\" PARAMETER whisper_logprob_thold -1.0 #logprob_thold PARAMETER whisper_no_speech_thold 0.6 #no_speech_thold You can now use the Cool-Whisper model for speech recognition. The following figure shows the use of Whisper and Cool-Whisper for recognizing mixed Chinese-English audio files, which can accurately recognize mixed Chinese-English scenarios","title":"Setup Steps"},{"location":"tutorial/2024-07-18-cool-whisper-tutorial/#references","text":"Cool-Whisper's HuggingFace Hub Professor Lee's Facebook post","title":"References"},{"location":"tutorial/client_library/","text":"Kuwa Client Library This Python library simplifies interaction with the Kuwa GenAI OS APIs. [!WARNING] This library is currently under active development and its interface may change in future releases. Installation cd genai-os/src/library/client pip install . Examples Setup First, import the necessary class and instantiate the client: from kuwa.client import KuwaClient client = KuwaClient( base_url=\"http://localhost\", kernel_base_url=\"http://localhost:9000\", model=\"geminipro\", auth_token=\"YOUR_API_TOKEN_HERE\" ) This code snippet imports the KuwaClient and creates an instance configured to connect to a local Kuwa GenAI OS instance using the \"geminipro\" model. Remember to replace \"YOUR_API_TOKEN_HERE\" with your actual API token. Using the Chat API The following example demonstrates how to use the chat completion feature: from kuwa.client import KuwaClient import asyncio messages = [ {\"role\": \"user\", \"content\": \"Hi\"} ] async def main(): async for chunk in client.chat_complete(messages=messages): print(chunk, end='') print() asyncio.run(main()) This example defines a list of messages representing a conversation and uses the chat_complete method to stream responses from the model. The async for loop iterates over chunks of the response and prints them as they arrive. Creating a Base Model This example shows how to create a new base model: from kuwa.client import KuwaClient import asyncio async def main(): try: response = await client.create_base_model( name=\"API_TEST_MODEL\", access_code=\"API_TEST_MODEL\", order=1, ) print(\"Model created:\", response) except Exception as e: print(\"Failed to create model:\", e) asyncio.run(main()) Here, the create_base_model method sends a request to create a new base model with the specified name, access code, and order. Creating a Bot with a Base Model This example demonstrates creating a new bot and linking it to a previously created base model: from kuwa.client import KuwaClient import asyncio async def main(): try: response = await client.create_base_model( name=\"API_TEST_MODEL\", access_code=\"API_TEST_MODEL\", order=1, ) print(\"Model created:\", response) response = await client.create_bot( bot_name=\"API_TEST_MODEL_BOT\", llm_access_code=\"API_TEST_MODEL\", ) print(\"Bot created:\", response) except Exception as e: print(\"An error occurred:\", e) asyncio.run(main()) This example first creates a new base model and then uses the create_bot method to create a new bot that utilizes the newly created base model. These examples showcase the basic usage of the Kuwa Client Library. For more advanced features and detailed documentation, please refer to the official documentation.","title":"Client library"},{"location":"tutorial/client_library/#kuwa-client-library","text":"This Python library simplifies interaction with the Kuwa GenAI OS APIs. [!WARNING] This library is currently under active development and its interface may change in future releases.","title":"Kuwa Client Library"},{"location":"tutorial/client_library/#installation","text":"cd genai-os/src/library/client pip install .","title":"Installation"},{"location":"tutorial/client_library/#examples","text":"","title":"Examples"},{"location":"tutorial/client_library/#setup","text":"First, import the necessary class and instantiate the client: from kuwa.client import KuwaClient client = KuwaClient( base_url=\"http://localhost\", kernel_base_url=\"http://localhost:9000\", model=\"geminipro\", auth_token=\"YOUR_API_TOKEN_HERE\" ) This code snippet imports the KuwaClient and creates an instance configured to connect to a local Kuwa GenAI OS instance using the \"geminipro\" model. Remember to replace \"YOUR_API_TOKEN_HERE\" with your actual API token.","title":"Setup"},{"location":"tutorial/client_library/#using-the-chat-api","text":"The following example demonstrates how to use the chat completion feature: from kuwa.client import KuwaClient import asyncio messages = [ {\"role\": \"user\", \"content\": \"Hi\"} ] async def main(): async for chunk in client.chat_complete(messages=messages): print(chunk, end='') print() asyncio.run(main()) This example defines a list of messages representing a conversation and uses the chat_complete method to stream responses from the model. The async for loop iterates over chunks of the response and prints them as they arrive.","title":"Using the Chat API"},{"location":"tutorial/client_library/#creating-a-base-model","text":"This example shows how to create a new base model: from kuwa.client import KuwaClient import asyncio async def main(): try: response = await client.create_base_model( name=\"API_TEST_MODEL\", access_code=\"API_TEST_MODEL\", order=1, ) print(\"Model created:\", response) except Exception as e: print(\"Failed to create model:\", e) asyncio.run(main()) Here, the create_base_model method sends a request to create a new base model with the specified name, access code, and order.","title":"Creating a Base Model"},{"location":"tutorial/client_library/#creating-a-bot-with-a-base-model","text":"This example demonstrates creating a new bot and linking it to a previously created base model: from kuwa.client import KuwaClient import asyncio async def main(): try: response = await client.create_base_model( name=\"API_TEST_MODEL\", access_code=\"API_TEST_MODEL\", order=1, ) print(\"Model created:\", response) response = await client.create_bot( bot_name=\"API_TEST_MODEL_BOT\", llm_access_code=\"API_TEST_MODEL\", ) print(\"Bot created:\", response) except Exception as e: print(\"An error occurred:\", e) asyncio.run(main()) This example first creates a new base model and then uses the create_bot method to create a new bot that utilizes the newly created base model. These examples showcase the basic usage of the Kuwa Client Library. For more advanced features and detailed documentation, please refer to the official documentation.","title":"Creating a Bot with a Base Model"}]}