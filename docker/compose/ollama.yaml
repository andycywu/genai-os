services:
  ollama-executor:
    extends:
      file: executor_base.yaml
      service: executor-base
    depends_on:
      - executor-builder
    environment:
      EXECUTOR_TYPE: ollama
      EXECUTOR_ACCESS_CODE: llama3
      EXECUTOR_NAME: Llama3
      EXECUTOR_IMAGE: ollama.png # Refer to src/multi-chat/public/images
    command: [
      "--ollama_host", "host.docker.internal", # The ollama server on host
      "--model", "llama3"
    ]